{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "irqzKVH55WNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import all the main important **dependencies** "
      ],
      "metadata": {
        "id": "1aFsc5HPUO-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random, os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import shutil\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "\n",
        "def tf_seed(seed=0):\n",
        "  np.random.seed(seed)  # numpy seed\n",
        "  tf.random.set_seed(seed)  # tensorflow seed\n",
        "  random.seed(seed)  # random seed\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
        "  os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  \n",
        "\n",
        "\n",
        "tf_seed()\n",
        "\n",
        "#see https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development"
      ],
      "metadata": {
        "id": "eoiW3DW47srm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main idea is to classify a **bonafide or attack** human face with a fast **CNN**, setting on the top of the network an **SSD** model, pretrained on a **ResNet**, that can extract faces from images.\n",
        "\n",
        "The network will have two main convolutional steps, in which we are going to extract deep features from the second phase. It uses in sequence **Conv2d + BatchNormalization** to learn faster, normalizing batch feature maps to avoid **overfitting**. However, this is why is important to set a right batch size and training data to avoid the opposite problem, then underfitting. The BatchNormalization, since this is a shallow network, won't have the so called vanishing gradient problem during the first training epochs.\n",
        "\n",
        "There are also, at the end of each phase, a Dropout layer. This is because it's important to learn as much details as possible to distinguish a \"real or fake\" person in front of a camera. So switching off weights during training randomly, will have the consequence to distribute better the learning of important features among all the weights."
      ],
      "metadata": {
        "id": "_UxygEbcUYe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEFINE NETWORK"
      ],
      "metadata": {
        "id": "e23Y0eD3K0Q7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Originary Network"
      ],
      "metadata": {
        "id": "e8JXHbf1NAkl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PARxPl2btms"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(input)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  #to avoid overfitting \n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AttackNet v2.2"
      ],
      "metadata": {
        "id": "Tmuw_6DNNEzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  y = tf.keras.layers.Conv2D(16, 3, padding = 'same')(input)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "  z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.add([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  y = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.add([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(128, activation = 'tanh')(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ],
      "metadata": {
        "id": "cIbyozQ9peu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3DMAD DATASET"
      ],
      "metadata": {
        "id": "XF972weGK-zC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is the following: https://www.idiap.ch/en/dataset/3dmad/index_html\n",
        "\n",
        "Nesli Erdogmus and Sébastien Marcel, \"Spoofing in 2D Face Recognition with 3D Masks and Anti-spoofing with Kinect\", Biometrics: Theory, Applications and Systems, 2013.\n",
        "10.1109/BTAS.2013.6712688\n",
        "https://publications.idiap.ch/index.php/publications/show/2657\n",
        "\n",
        "<br>The 3D Mask Attack Database (3DMAD) is a biometric (face) spoofing database. We get a subset of data. In our case it contains frames of 17 persons, recorded using Kinect for both real-access and spoofing attacks. Each frame consists of:\n",
        "\n",
        "*   a depth image (640x480 pixels – 1x11 bits)\n",
        "*   the corresponding RGB image (640x480 pixels – 3x8 bits)\n",
        "\n",
        "<br>The data is collected in **3 different sessions** for all subjects and for each session 5 videos of 300 frames are captured. The recordings are done under controlled conditions, with frontal-view and neutral expression. The first two sessions are dedicated to the real access samples, in which subjects are recorded with a time delay of ~2 weeks between the acquisitions. In the third session, 3D mask attacks are captured by a single operator (attacker).\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "## Training and Validation Data\n",
        "\n",
        "We define **which subject folders** to use in **a certain session** for **validation data**. **Training data** will be equal to the remaining data.\n",
        "\n",
        "Setting the variable **split_randomly_training_and_validation**\n",
        "to **true** will **ignore** validation chosen data, and **will split the entire dataset in 70% for training, 30% for validation. It is not recommended, but you can check how easy is to reach a good validation accuracy, since there are subjects for testing that appear also on training**."
      ],
      "metadata": {
        "id": "KqP77ScjVv-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Root Folders"
      ],
      "metadata": {
        "id": "lC86JJ3fNwfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_folder=\"/content/drive/MyDrive/liveness_project/\"\n",
        "\n",
        "\n",
        "protoPath=os.path.join(root_folder,'face_detector/deploy.prototxt.txt')\n",
        "modelPath = os.path.join(root_folder,\n",
        "\t\"face_detector/res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "\n",
        "face_confidence=0.68\n",
        "\n",
        "root_3DMAD_extracted=os.path.join(root_folder,'dataset/training/3DMAD extracted/')\n",
        "\n",
        "all_bonifade_folders=[1,2] #session 1,2\n",
        "all_attack_folders=[3] #session 3\n",
        "\n",
        "# here for training we use all the folders except that used for validation\n",
        "attack_validation_labels=[[3,[7, 10,11,12,13,14]]] #from session 3, take subjects n. 10,11,12,13,14 folders for attack validation\n",
        "bonifade_validation_labels=[[1,[7, 10,11,12,13,14]],[2,[7, 10,11,12,13,14]]] #from session 1 and 2, take subjects n. 10,11,12,13,14 folders for bonifade validation\n",
        "\n",
        "split_randomly_training_and_validation=False\n",
        "\n",
        "#will ignore validation data since all dataset will be splitted in 70% for training and 30% for validation\n",
        "if split_randomly_training_and_validation:\n",
        "\tattack_validation_labels=[]\n",
        "\tbonifade_validation_labels=[]\n",
        "\n",
        "\n",
        "\n",
        "#we define where to save output detected faces and the corresponding depth\n",
        "output_attack_training_folder=os.path.join(root_folder,'dataset/training/3DMAD/images/attack_training/')\n",
        "output_attack_validation_folder=os.path.join(root_folder,'dataset/training/3DMAD/images/attack_validation/')\n",
        "\n",
        "output_bonifade_training_folder=os.path.join(root_folder,'dataset/training/3DMAD/images/bonifade_training/')\n",
        "output_bonifade_validation_folder=os.path.join(root_folder,'dataset/training/3DMAD/images/bonifade_validation/')\n",
        "\n",
        "\n",
        "save_augmented_images =  os.path.join(root_folder,'dataset/training/3DMAD/augmented_images/') #output of online augmented images \n",
        "save_model_h5 = os.path.join(root_folder,'model_3DMAD.h5') \n",
        "save_labels = os.path.join(root_folder,'model_3DMAD_labels')\n",
        "save_training_metrics_plot = os.path.join(root_folder,'3DMAD_plot.png')\n",
        "\n",
        "output_np_training_array = os.path.join(root_folder,'3DMAD_training_array')\n",
        "\n",
        "\n",
        "\n",
        "def check_subject_folder(subjects, folder_name):\n",
        "\n",
        "\tfor subject in subjects:\n",
        "\t\tif int(folder_name.split(\"_\")[0]) == subject:\n",
        "\t\t\treturn True\n",
        "\n",
        "\treturn False\n",
        "\n",
        "#is_validation needs to understand if we are taking folders for validation or training\n",
        "#validation_folders needs when we are taking data for training: we get all the folders - validation folders in this case\n",
        "def get_folders(is_validation=True, labels=None, validation_folders=None):\n",
        "\tfolders=[]\n",
        "\tnew_directories=[]\n",
        "\n",
        "\tif is_validation:\n",
        "\n",
        "\t\tfor label in labels:\n",
        "\t\t\tnew_root = os.path.join(root_3DMAD_extracted,\"session \" + (str(label[0]) ))\n",
        "\t \n",
        "\t\t\tsubjects = label[1]\n",
        "\n",
        "\t\t\tnew_directories += [f.path for f in os.scandir(new_root) if f.is_dir() and check_subject_folder(subjects, f.name)]\n",
        "\n",
        "\telse:\n",
        "\t\t#for training we use all the session folders - validation folders\n",
        "\t\tfor label in labels:\n",
        "\t\t\tnew_root = os.path.join(root_3DMAD_extracted,\"session \" + str(label))\n",
        "\t \n",
        "\t\t\tnew_directories += [f.path for f in os.scandir(new_root) if f.is_dir()]\n",
        "\t\t\tnew_directories = list(set(new_directories) - set(validation_folders))\t#exclude subjects used for testing\n",
        "\t\t \n",
        "\n",
        "\t\t\t\t\n",
        "\tfor directory in new_directories:\n",
        "\t\t\t\tsublist=[]\n",
        "\t\t\t\tsublist.append(os.path.join(directory, \"Color_Data\"))\n",
        "\t\t\t\tsublist.append(os.path.join(directory, \"Depth_Data\"))\n",
        "\t\t\t\tfolders.append(sublist)\n",
        "\t\t\n",
        "\n",
        "\t\n",
        "\tif is_validation:\n",
        "\t\t\treturn folders, new_directories\n",
        "\t\n",
        "\treturn folders\n",
        "\n",
        "\n",
        "attack_folders_validation, attack_validation_folders=get_folders(labels=attack_validation_labels)\n",
        "attack_folders_training = get_folders(labels=all_attack_folders, is_validation=False, validation_folders=attack_validation_folders)\n",
        "\n",
        "\n",
        "bonifade_folders_validation, bonifade_validation_folders=get_folders(labels=bonifade_validation_labels)\n",
        "bonifade_folders_training = get_folders(labels=all_bonifade_folders, is_validation=False, validation_folders=bonifade_validation_folders)\n",
        "\n",
        "\n",
        "training_directories=[attack_folders_training, attack_folders_validation, bonifade_folders_training, bonifade_folders_validation]\n",
        "output_directories=[output_attack_training_folder, output_attack_validation_folder, output_bonifade_training_folder, output_bonifade_validation_folder]\n",
        "\n",
        "\n",
        "#create directories where to save images if they don't exist\n",
        "for output_dir in output_directories:\n",
        "\tif not os.path.exists(output_dir): \n",
        "\t  os.makedirs(output_dir)\n",
        "\t  print(\"created \", output_dir)\n",
        "\t \n",
        "\tmask_dir = os.path.join(output_dir ,\"mask\")\n",
        "\tif not os.path.exists(mask_dir): \n",
        "\t  os.makedirs(mask_dir)\n",
        "\t \n",
        "print(\"Attack folders for validation: \" , attack_folders_validation)\n",
        "print(\"Attack folders for training: \" , attack_folders_training)\n",
        "\n",
        "print(\"Bonifade folders for validation: \" , bonifade_folders_validation)\n",
        "print(\"Bonifade folders for training: \" , bonifade_folders_training)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZZXrDHVfVAOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NETWORK PARAMETERS FOR TRAINING"
      ],
      "metadata": {
        "id": "7HE6q3yKLb16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_x = 32\n",
        "final_y = 32\n",
        "\n",
        "n_channels = 3\n",
        "\n",
        "dim = (final_x, final_y)\n",
        "\n",
        "epochs=20\n",
        "batch_size=16\n",
        "init_learning_rate= 1e-5\n",
        "\n",
        "#return normalized image\n",
        "def resize_normalize_image(image, value=255):\n",
        "  image=cv2.resize(image, dim)\n",
        "  return (image)/value"
      ],
      "metadata": {
        "id": "La8H1nBNILps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FACE PROCESSING"
      ],
      "metadata": {
        "id": "dzeH6X9BLKLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network to Detect Faces\n",
        "\n",
        "We define the SSD network that detects face inside an image.\n",
        "\n",
        "In this case, since depth mask is not aligned with the original rgb image, we need to take the aligned color to depth image for each color image, in this way we can then concatenate the depth to the rgb channels."
      ],
      "metadata": {
        "id": "LjPSQ7cRG9-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "\n",
        "def im_show(image, size=(15,15), output=None):\n",
        "   fig = plt.figure(figsize=size) # set the height and width in inches\n",
        "   plt.imshow(image, cmap=\"Greys\", interpolation=\"nearest\")\n",
        "   plt.axis(\"off\")\n",
        "   plt.show()\n",
        "\n",
        "   if output is not None:\n",
        "     cv2.imwrite(output, image)\n",
        "\n",
        "# detect the most probable face, or all of detected face\n",
        "# return coordinates of bounding box for each face and the face itself as a cv2 object\n",
        "def detect_save_face(color, depth, output_path=None, mask_output=None, multiple_output=False):\n",
        "\n",
        "    (h, w) = color.shape[:2] \n",
        "\n",
        "\n",
        "    # we preprocess image normalizing each r-g-b channel subtracting the values in the last tuple\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(color, (300, 300)), 1.0,\n",
        "                                 (300, 300), (104.0, 177.0, 123.0))\n",
        "    \n",
        "\n",
        "    # pass the blob through the network and obtain the detections and\n",
        "    # predictions\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "\n",
        "    faces = []\n",
        "    coordinates=[]\n",
        "\n",
        "\n",
        "    # ensure at least one face was found\n",
        "    if  len(detections) > 0:\n",
        "\n",
        "    \n",
        "        max_i=np.argmax(detections[0, 0, :, 2]) #this is the most probable detected face\n",
        "\n",
        "        #if we want to output the most probable detected face or all the faces\n",
        "        min_range = 0 if multiple_output==True else max_i \n",
        "        max_range = detections.shape[2] if multiple_output==True else max_i + 1\n",
        "\n",
        "        for i in range(min_range, max_range):\n",
        "\n",
        "             # we get the model confidence\n",
        "              confidence = detections[0, 0, i, 2]\n",
        "\n",
        "              if confidence >= face_confidence:\n",
        "               \n",
        "                # compute the (x, y)-coordinates of the bounding box for\n",
        "                # the face and extract the face ROI\n",
        "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "               \n",
        "                # we save coordinates \n",
        "                coordinates.append(box.astype(\"int\"))\n",
        "                face = color[startY:endY, startX:endX]\n",
        "                face_mask = depth[startY:endY, startX:endX]\n",
        "\n",
        "                \n",
        "\n",
        "                #we save the face\n",
        "                faces.append(face)\n",
        "\n",
        "                              \n",
        "                if mask_output is not None:\n",
        "                     np.save(mask_output, face_mask)\n",
        "      \n",
        "                if output_path is not None:\n",
        "                    cv2.imwrite(output_path, face)\n",
        "                    \n",
        "             \n",
        "\n",
        "    return faces, coordinates\n",
        "\n",
        "\n",
        "#return file name given a path\n",
        "def get_file_name(path):\n",
        "    base = os.path.basename(path)\n",
        "    return os.path.splitext(base)[0]\n",
        "\n",
        "#return file extension given a path\n",
        "def get_file_type(path): \n",
        "    base = os.path.basename(path)\n",
        "    return os.path.splitext(base)[1] # ex. .png, .jpg\n",
        "\n"
      ],
      "metadata": {
        "id": "JNYKW76cU8KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract Faces from Images\n",
        "\n",
        "We are going to take videos or images from our dataset. For each frame will be detected **one** face, and this one will be saved to the output folders defined before. The code reserves the possibility for the user to define also a mask for each image. In this case, the detected face will be also \"cropped\" from the mask."
      ],
      "metadata": {
        "id": "43yAly1EYYXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_images_per_folder=200000 #we want to take a maximum number of images for training since limited hardware resources\n",
        "\n",
        "\n",
        "#for each training input directory, search for color image\n",
        "#for each color image (RGB), we take the corresponding align_to_depth and depth images \n",
        "#for each image and depth, we detect one face\n",
        "def take_images():\n",
        "    directory_counter = 0\n",
        "    file_counter = 0\n",
        "\n",
        "    for main_list in training_directories:\n",
        "\n",
        "          folder_file_counter=0\n",
        "\n",
        "          d_index=0\n",
        "\n",
        "          print(\"we're on array %d/%d\" % (directory_counter + 1, len(training_directories)))\n",
        "\n",
        "\n",
        "          for directory in main_list:\n",
        "\n",
        "            d_index+=1\n",
        "\n",
        "          # files = [file for file in os.listdir(directory[0])]\n",
        "\n",
        "            list_images = list(paths.list_images(directory[0]))\n",
        "\n",
        "            print(\"found %d images for folder n. %d/%d\" % (len(list_images), d_index, len(main_list)))\n",
        "\n",
        "\n",
        "            #we shuffle training data since we just take a limited number of images\n",
        "            random.Random(20).shuffle(list_images)\n",
        "\n",
        "\n",
        "            for file_path in list_images:\n",
        "\n",
        "                if folder_file_counter > max_images_per_folder:\n",
        "                  break\n",
        "\n",
        "              #  file_path = os.path.join(directory, file)\n",
        "                \n",
        "                file_name=get_file_name(file_path)\n",
        "                try:\n",
        "                  depth_image=np.load(os.path.join(directory[1], file_name + \".npy\"))\n",
        "                except:\n",
        "                  continue\n",
        "                \n",
        "                color=cv2.imread(file_path)\n",
        "\n",
        "              \n",
        "\n",
        "                detect_save_face(color, depth_image, (output_directories[directory_counter] + \"%s%s\" % (\n",
        "                        \"frame_%d\" % file_counter, get_file_type(file_path))), (output_directories[directory_counter] + \"/mask/%s%s\" % (\n",
        "                        \"frame_%d_mask\" % file_counter, \".npy\")))\n",
        "                    \n",
        "                folder_file_counter+=1\n",
        "\n",
        "                print(\"save face detected on image n. %d, for folder n. %d\" % (file_counter, directory_counter + 1))\n",
        "\n",
        "\n",
        "                file_counter += 1\n",
        "\n",
        "          directory_counter += 1\n",
        "\n",
        "\n",
        "take_images() "
      ],
      "metadata": {
        "id": "3QPqFZrOtse2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# TRAINING SECTION"
      ],
      "metadata": {
        "id": "EfcLzIHfNhAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Training and Validation Data\n",
        "\n",
        "For each directory where we saved detected faces, we are going to create two arrays: images, that will be concatenated with the depth mask, **by default**, and labels (0 for bonafide, 1 for attackers)\n"
      ],
      "metadata": {
        "id": "C6t_3xFfYdr6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_images(directories, mask=True, balance_dataset=True, undersampling=False):\n",
        "\n",
        "    image_list = []\n",
        "    label_list = []\n",
        "\n",
        "    classes = 0\n",
        "\n",
        "    images_per_class=[]\n",
        "\n",
        "    tf_seed()\n",
        "\n",
        "    #for each input directory\n",
        "    for i in directories:\n",
        "\n",
        "        list_images = [f for f in os.listdir(i) if os.path.isfile(os.path.join(i, f))]\n",
        "\n",
        "        images_number = len(list_images)\n",
        "\n",
        "        random.Random(20).shuffle(list_images)\n",
        "\n",
        "        print(\"n %d\" % images_number)\n",
        "\n",
        "        #if we use undersampling\n",
        "        if undersampling:\n",
        "          images_number = images_number if ((len(images_per_class) == 0) or (images_number <= images_per_class[-1]))  else images_per_class[-1]\n",
        "        \n",
        "        \n",
        "        # to fill tensors we inizialise them\n",
        "        X = np.empty((images_number, final_x, final_y, n_channels))\n",
        "\n",
        "        L = np.empty((images_number))\n",
        "\n",
        "        ipp = 0\n",
        "\n",
        "        for im_name in list_images:\n",
        "            \n",
        "            im=os.path.join(i, im_name)\n",
        "\n",
        "\n",
        "            # to get \"balanced dataset\" using undersampling method\n",
        "            if (balance_dataset and undersampling) and len(images_per_class)  and ipp == images_per_class[-1]:\n",
        "                break\n",
        "\n",
        "            image = cv2.imread(im)\n",
        "\n",
        "        \n",
        "\n",
        "            image=resize_normalize_image(image, value=255)\n",
        "\n",
        "          \n",
        "            if mask:\n",
        "              mask_name = os.path.join(\n",
        "                    i,\"mask\", get_file_name(im) + \"_mask.npy\")\n",
        "              image_mask = np.load(mask_name)\n",
        "              image_mask = resize_normalize_image(image_mask, value=2048)  #since 3DMAD mask is rgb 2^11\n",
        "              \n",
        "                        \n",
        "\n",
        "            X[ipp, ..., :3] = image # each original image is rgb\n",
        "            \n",
        "            if mask:\n",
        "              X[ipp,...,3] = image_mask # each mask can be n_channels - 3\n",
        "\n",
        "            L[ipp] = classes  # 0 for real, 1 for fake for binary classification\n",
        "\n",
        "            ipp += 1\n",
        "        \n",
        "        # check if the current class has lower images than the before\n",
        "        # in this case we undersample the majority class randomly removing elements\n",
        "        # we will have a balanced dataset: same images for all the classes\n",
        "\n",
        "        if balance_dataset and undersampling:\n",
        "            if len(images_per_class) and ipp < images_per_class[-1]:\n",
        "            \n",
        "              left_shift = 0\n",
        "\n",
        "              for i in range(0, len(images_per_class)):\n",
        "\n",
        "                  diff = images_per_class[i] - ipp\n",
        "\n",
        "                  for j in range(0, diff):\n",
        "                      random_index = np.random.randint(left_shift, images_per_class[i] - j)\n",
        "                      image_list.pop(random_index)\n",
        "                      label_list.pop(random_index)\n",
        "\n",
        "                  left_shift += images_per_class[i] - diff\n",
        "\n",
        "        if undersampling or balance_dataset==False:\n",
        "              for image in X:\n",
        "                image_list.append(image)\n",
        "\n",
        "              for label in L:\n",
        "                label_list.append(label)\n",
        "        else:\n",
        "              image_array=[]\n",
        "              label_array=[]\n",
        "            \n",
        "              for image in X:\n",
        "                image_array.append(image)\n",
        "\n",
        "              image_list.append(np.array(image_array, dtype=\"float\"))\n",
        "\n",
        "             \n",
        "              for label in L:\n",
        "                label_array.append(label)\n",
        "\n",
        "              label_list.append(label_array)\n",
        "\n",
        "              #[[2k], [1k]]\n",
        "        \n",
        "        if undersampling or balance_dataset==False:\n",
        "            images_per_class.append(ipp)\n",
        "        else:\n",
        "            images_per_class.append(images_number)\n",
        "\n",
        "        classes += 1\n",
        "\n",
        "    #offline augmentation\n",
        "    if balance_dataset and undersampling==False:\n",
        "\n",
        "      \n",
        "         max=np.max((images_per_class)) #[4k,3k,2k,1k]\n",
        "\n",
        "         for j in range(0, len(images_per_class)):  #[[4k], [4k], [4k], [4k]]\n",
        "            if images_per_class[j] < max:\n",
        "              diff=max - images_per_class[j] #how many images to generate\n",
        "\n",
        "              image_array=image_list[j]   #[3k]\n",
        "              label_array=label_list[j]   #[3k]\n",
        "\n",
        "              new_image_array=[]\n",
        "             \n",
        "\n",
        "              #use ImageDataGenerator \n",
        "              offline_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                       \twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\t                       horizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "              offline_generator=offline_generator.flow(image_array, batch_size=1, seed = 20)\n",
        "\n",
        "              #generate the reamining images respect the majority class, to balance the dataset\n",
        "              generated_images=0\n",
        "              while generated_images < diff:\n",
        "                     \n",
        "                  for i in range(0,len(offline_generator)):\n",
        "                        if generated_images == diff:\n",
        "                          break\n",
        "                        batch= next(offline_generator)\n",
        "                        # print(batch.shape)\n",
        "                        #print(new_image_array.shape)\n",
        "                        new_image_array.append(batch[0])\n",
        "                        \n",
        "                        label_array.append(j)\n",
        "\n",
        "                      \n",
        "                        generated_images += 1\n",
        "\n",
        "              print(generated_images)\n",
        "     \n",
        "              image_list[j]=np.append(image_list[j], new_image_array, axis=0)\n",
        "\n",
        "\n",
        "         # image_list and label_list were of type [[...], [...]] for attack and bonafide classes\n",
        "         # we need a final array of type [image1,image2,...] for images and [0,1,...] for labels\n",
        "\n",
        "         new_image_list= np.empty((0, final_x, final_y, n_channels))\n",
        "         new_label_list=[]\n",
        "\n",
        "\n",
        "         for class_value_array in image_list:\n",
        "           new_image_list = np.vstack((new_image_list,class_value_array))\n",
        "\n",
        "         for class_value_array in label_list:\n",
        "           new_label_list += class_value_array\n",
        "\n",
        "         image_list = new_image_list\n",
        "         label_list = new_label_list\n",
        "\n",
        "         print(image_list.shape)\n",
        "         print(len(label_list))\n",
        "         \n",
        "        # im_show(new_image_list[2]) # we can show up an image \n",
        "      \n",
        "        \n",
        "    #create the one-hot encoded vector: [0 1], [1 0]\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(label_list)\n",
        "    labels = tf.keras.utils.to_categorical(labels, 2)\n",
        "\n",
        "    if type(image_list) is np.ndarray:\n",
        "      return image_list, labels, le\n",
        "\n",
        "    return  np.array(image_list, dtype=\"float\"), labels, le\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "read_from_np_array = True\n",
        "\n",
        "\n",
        "## WE CAN SAVE NUMPY RESULTS AND REUSE THEM LATER ##\n",
        "\n",
        "if read_from_np_array:\n",
        "      (train_X, test_X, train_Y, test_Y) = pickle.loads(open(output_np_training_array, \"rb\").read())\n",
        "      le = pickle.loads(open(save_labels, \"rb\").read())\n",
        "\n",
        "## WE ARE GOING TO GET THE TRAIN AND VALIDATION DATASETS, AND WE SAVE THE NUMPY ARRAYS ##\n",
        "else:\n",
        "      \n",
        "      train_X, train_Y, le = get_images([output_bonifade_training_folder, output_attack_training_folder], undersampling=True, mask = False)\n",
        "\n",
        "     \n",
        "      #will be ignored if we set split_randomly_training_and_validation variable to True\n",
        "      test_X, test_Y, _ = get_images([output_bonifade_validation_folder, output_attack_validation_folder], undersampling=True, mask = False)\n",
        "\n",
        "\n",
        "      #save model labels \n",
        "      f = open(save_labels, \"wb\")\n",
        "      f.write(pickle.dumps(le))\n",
        "      f.close()\n",
        "\n",
        "      #save results\n",
        "      pickle.dump((train_X, test_X, train_Y, test_Y), open(output_np_training_array, 'wb'))\n",
        "\n",
        "\n",
        "#will split entire dataset in 80% for training and 30% for validation\n",
        "if split_randomly_training_and_validation:\n",
        "\n",
        "  #the random seed for splitting \n",
        "\tsplit_seed=20\n",
        "\n",
        "\ttrainX, testX, trainY, testY = train_test_split(train_X, train_Y,\n",
        "\ttest_size=0.3, stratify=train_Y, seed=split_seed)\n",
        " \n",
        "else:\n",
        "\ttrainX, testX, trainY, testY = (train_X, test_X, train_Y, test_Y)\n",
        " \n",
        "print(\"Shape training: \", trainX.shape)\n",
        "print(\"Shape validation: \", testX.shape)\n"
      ],
      "metadata": {
        "id": "izwSR4HW0zbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Augmentation Online"
      ],
      "metadata": {
        "id": "u_lCe8-YMFIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "augmentation_online=True\n",
        "\n",
        "\n",
        "\n",
        "#the random seed for online augmentation\n",
        "generator_seed=10\n",
        "\n",
        " \n",
        "if augmentation_online:\n",
        "\t\t#online augmentation of training images\n",
        "\t\taug_training = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=30, zoom_range=0.25,\n",
        "\t\t\twidth_shift_range=0.3, height_shift_range=0.3, shear_range=0.15,\n",
        "\t\t\thorizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "\t\t#aug_training = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "\n",
        "\t\t# output an endless Iterator of batches\n",
        "\t\t# shuffle images, so the first generated batch is from batch_size random training images\n",
        "\n",
        "\t\ttrain_generator=aug_training.flow(trainX, trainY, batch_size=batch_size, shuffle=True, seed=generator_seed)\n",
        "\n",
        "\n",
        "\t\t# will be equal to np.ceil(len(train(X))/batch_size)\n",
        "\t\t# notice: if len(trainX) / batch_size is a float value, the last batch will contain less images than batch_size value\n",
        "\t\tprint(\"lenght training batches \", len(train_generator))\n",
        "\t\tprint(\"lenght training data \", len(trainX))\n",
        "\t\tprint(\"lenght validation data \", len(testX))\n",
        "\n",
        "\t\t# we could use also a validation_generator, without augment images\n",
        "\t\t# this \"trick\" could be done for set up a valdation batch size on model.fit().. then at the end of an epoch, each batch will be validated\n",
        "\n",
        "\t\t#aug_validation=ImageDataGenerator()\n",
        "\t\t#validation_generator=aug_validation.flow(testX, testY, batch_size=batch_size, shuffle=False, seed=1)"
      ],
      "metadata": {
        "id": "vz29KTXlJJIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can output the generator images organized in batches"
      ],
      "metadata": {
        "id": "kp8mvCvwLH5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# we generate batch_size images, beginning from the first batch\n",
        "batchx, batchy = next(train_generator)\n",
        "for i in range(batchx.shape[0]):\n",
        "    fig, (ax1) = plt.subplots(1) \n",
        "    ax1.set_title('image')  \n",
        "    ax1.imshow(batchx[i][:,:,0], cmap='gray')\n",
        "    print(\"Label-%d:\"%i, batchy[i][0])\n",
        "\n",
        "\n",
        "train_generator.reset() #once an epoch ends, this function will be called to reset data\n",
        "\n",
        "# we generate exactly len(trainX) - batch_size images beginning from the first batch, since we have reset the train_generator on the line before\n",
        "for i in range(0,len(train_generator) - 1):\n",
        " break # remove to see all training augmented images for an epoch\n",
        " batchx, batchy = next(train_generator)\n",
        " for i in range(batchx.shape[0]):\n",
        "    fig, (ax1) = plt.subplots(1) \n",
        "    ax1.set_title('image')  \n",
        "    ax1.imshow(batchx[i][:,:,0], cmap='gray')\n",
        "    print(\"Label-%d:\"%i, batchy[i][0])\n",
        "\n"
      ],
      "metadata": {
        "id": "uE71Q14r1oRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OFFLINE AUGMENTATION"
      ],
      "metadata": {
        "id": "rtE7tE8ezNw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each image in the training set, we're going to augment the image itself and we add it to the training. So if we have 2k images for the training, the final length of the dataset will be 4k."
      ],
      "metadata": {
        "id": "GVBP6_3baHSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "augmentation_online=False\n",
        "\n",
        "generator_seed=10\n",
        "     \n",
        "\n",
        "if not augmentation_online:\n",
        "\n",
        "\n",
        "  to_augment=len(trainY) * 2\n",
        "\n",
        "  #print(to_augment)\n",
        "  # to fill tensors we inizialise them\n",
        "  X = np.empty((to_augment, final_x, final_y, n_channels), dtype=\"float\")\n",
        "\n",
        "  L = np.empty((to_augment), dtype=\"int\")\n",
        "\n",
        "  #use ImageDataGenerator \n",
        "  offline_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=30, zoom_range=0.25,\n",
        "\t\t\twidth_shift_range=0.3, height_shift_range=0.3, shear_range=0.15,\n",
        "\t\t\thorizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "\n",
        "  offline_generator=offline_generator.flow(trainX, trainY, shuffle=True, batch_size=1, seed=generator_seed)\n",
        "\n",
        "  #for all initial images\n",
        "  for i in range(0,to_augment):\n",
        "            batchx, batchy = next(offline_generator)\n",
        "            X[i] = batchx[0]\n",
        "            L[i] = np.argmax(batchy[0])\n",
        "          \n",
        "  \n",
        "  labels = le.fit_transform(L)\n",
        "  labels = tf.keras.utils.to_categorical(labels, 2)\n",
        "\n",
        "  trainX = np.vstack((trainX, X))\n",
        "  trainY = np.vstack((trainY, labels))\n",
        "\n",
        "\n",
        "print(\"Shape training: \", trainX.shape)\n",
        "print(\"Shape validation: \", testX.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "HvXvTIxcXnOb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "252dd2df-3240-444a-e126-16b9a9ab761f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape training:  (3960, 32, 32, 3)\n",
            "Shape validation:  (720, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the Model**\n",
        "\n",
        "We train the **model** and we save in a **CustomCallback** instance the average loss and accuracy for each training and validation batch. In this way, we can create a more defined training plot."
      ],
      "metadata": {
        "id": "12hTjbEGYlba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch, psutil\n",
        "\n",
        "\n",
        "class MemoryUsage(tf.keras.callbacks.Callback):\n",
        "\n",
        "   def __init__(self):\n",
        "      # setting device on GPU if available, else CPU\n",
        "      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      print('Using device for training:', self.device)\n",
        "      self.max_RAM=[]\n",
        "      self.max_GPU=[]\n",
        "\n",
        "   def get_memory_usage(self):\n",
        "      gpu_dict = tf.config.experimental.get_memory_info('GPU:0')\n",
        "      tf.print('\\n GPU memory details [current: {} gb, peak: {} gb]'.format(\n",
        "          float(gpu_dict['current']) / (1024 ** 3), \n",
        "          float(gpu_dict['peak']) / (1024 ** 3)))\n",
        "   \n",
        "   def get_size(self, byte, suffix=\"GB\"):\n",
        "    factor = 1024\n",
        "    \n",
        "    for unit in [\"\", \"K\", \"M\", \"GB\", \"T\", \"P\"]:\n",
        "        if byte < factor:\n",
        "            return f\"{byte:.2f} GB\"\n",
        "        byte /= factor\n",
        "\n",
        "   def on_train_end(self, epoch, logs=None):\n",
        "      i=np.argmax(self.max_RAM)\n",
        "      j=np.argmin(self.max_RAM)\n",
        "      self.get_memory_usage()\n",
        "      print(\"MAX RAM USAGE: %s / %s (%s)\" % (self.get_size(self.max_RAM[i][0]), self.get_size(self.max_RAM[i][1]), str(self.max_RAM[i][2]) + \"%\" ))\n",
        "      \n",
        "\n",
        "\n",
        "   \n",
        "   def on_epoch_end(self,epoch,logs=None):\n",
        "      svmem = psutil.virtual_memory()\n",
        "      self.max_RAM.append((svmem.active, svmem.total, svmem.percent))\n",
        "      #self.get_memory_usage()\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.5)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.5)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate, decay=init_learning_rate/epochs)\n",
        "\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.99)\n",
        "tf_seed()\n",
        "model = LiveNet((final_y,final_x,n_channels))\n",
        "\n",
        "callback_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=False)\n",
        "memory_usage = MemoryUsage()\n",
        "\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])  # same of binary_crossentropy in this case\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#validation batch_size is equal to batch_size in this case (32)\n",
        "#check https://github.com/keras-team/keras/blob/v2.9.0/keras/engine/training.py#L1099-L1472\n",
        "if augmentation_online:\n",
        "    H=model.fit(\n",
        "      train_generator, \n",
        "      validation_data = (testX, testY), \n",
        "      callbacks = [memory_usage, callback_early_stopping],\n",
        "      epochs= epochs)\n",
        "else:\n",
        "   \n",
        "   H=model.fit(\n",
        "      trainX, trainY, \n",
        "      batch_size = batch_size,\n",
        "      shuffle=True,\n",
        "      validation_data = (testX, testY), \n",
        "      callbacks = [memory_usage, callback_early_stopping],\n",
        "      epochs= epochs)\n",
        "\n",
        "print(\"TRAINING TIME\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print((history.epoch_batch_train_acc[0])) #show first epoch batches averaged values\n",
        "\n",
        "print(\"[INFO] serializing network to '{}'...\".format(save_model_h5))\n",
        "model.save(save_model_h5, save_format=\"h5\")\n",
        "\n",
        "\n",
        "if not read_from_np_array:\n",
        "   #save model labels \n",
        "      f = open(save_labels, \"wb\")\n",
        "      f.write(pickle.dumps(label_encoder))\n",
        "      f.close()\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "fig=plt.figure(figsize=(12,7))\n",
        "\n",
        "\n",
        "\n",
        "new_epochs=len(H.history['loss'])\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "\n",
        "plt.savefig(save_training_metrics_plot)\n",
        "\n",
        "\n",
        "# notice accuracy at the end of each epoch is the mean all over the batches, the same if we have more than 1 validation batch\n",
        "\n",
        "#https://keras.io/guides/training_with_built_in_methods/\n",
        "#https://keras.io/getting_started/faq/#why-is-my-training-loss-much-higher-than-my-testing-loss\n",
        "#https://stackoverflow.com/questions/55097362/different-accuracy-by-fit-and-evaluate-in-keras-with-the-same-dataset\n",
        "\n",
        "# the below code can give never seen images at each validation_step; more in general we divide validation dataset in batches\n",
        "# see https://stackoverflow.com/questions/56991909/how-is-the-keras-accuracy-showed-in-progress-bar-calculated-from-which-inputs-i\n",
        "\n",
        "\n",
        "#model.fit(train_generator,\n",
        " # steps_per_epoch=len(train_generator),\n",
        "  #validation_data = validation_generator,\n",
        "\t#epochs=3)"
      ],
      "metadata": {
        "id": "NV09uMO-1jj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "GjlxdHfLMwi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#confusion matrix\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import itertools\n",
        "from itertools import cycle\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
        "\n",
        "model = tf.keras.models.load_model(save_model_h5)\n",
        "\n",
        "y_score = model.predict(test_X)\n",
        "\n",
        "y_pred = np.argmax(y_score, axis = 1)\n",
        "y_test_ = np.argmax(test_Y, axis = 1)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \n",
        "  \n",
        "    def precision(index):\n",
        "      return round(cm[index][index] / cm[:, index].sum(),2)\n",
        "    \n",
        "    def recall(index):\n",
        "      return round(cm[index][index] /cm[index].sum(),2)\n",
        "    \n",
        "    def F1_score(index):\n",
        "      p=precision(index)\n",
        "      r=recall(index)\n",
        "      return round((2 * p * r)/(p + r),2)\n",
        "    \n",
        "    plt.figure(figsize=(6, 6), dpi=80)\n",
        "\n",
        "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, round(cm[i, j],2),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        \n",
        "   \n",
        "\n",
        "    #### CREATE THE PRECISION / RECALL / F1_SCORE TABLE ####\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 2 + 3 / 2.5))\n",
        "    \n",
        "    col_labels=[\"Bonafide\", \"Attacker\"]\n",
        "    row_labels=['Precision','Recall','F1 Score']\n",
        "    row_func=[precision,recall,F1_score]\n",
        "    table_vals=[]  \n",
        "\n",
        "    row_colors = np.full(len(row_labels), 'linen')\n",
        "    col_colors = np.full(len(col_labels), 'lavender')\n",
        "\n",
        "    \n",
        "    for i in range(0, len(row_labels)):\n",
        "\n",
        "      row=[]\n",
        "\n",
        "      for j in range(0, len(col_labels)):\n",
        "         row.append(row_func[i](j))\n",
        "\n",
        "      table_vals.append(row)\n",
        "    \n",
        "\n",
        "    # the rectangle is where I want to place the table\n",
        "    table = plt.table(cellText=table_vals,\n",
        "                  cellLoc='center',\n",
        "                  rowColours=row_colors,\n",
        "                  rowLabels=row_labels,\n",
        "                  rowLoc='center',\n",
        "                  colColours=col_colors,\n",
        "                  colLabels=col_labels,\n",
        "                  loc='center')\n",
        "    table.scale(1, 2)\n",
        "    ax1.axis('off')\n",
        "\n",
        "class_names = [\"Bonafide\", \"Attacker\"]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_, y_pred)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion Matrix')\n",
        "\n",
        "\n",
        "# we're using SoftMax\n",
        "sigmoid=False\n",
        "\n",
        "# if you use sigmoid as output, you can print one ROC Curve\n",
        "if sigmoid:\n",
        "    #ROC CURVE\n",
        "\n",
        "    fpr, tpr, _ =  roc_curve(test_Y, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(5, 5), dpi=80)\n",
        "    lw = 2\n",
        "\n",
        "    plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "2SD2MTFiV-np"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}