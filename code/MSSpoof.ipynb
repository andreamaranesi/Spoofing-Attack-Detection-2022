{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irqzKVH55WNX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aFsc5HPUO-u"
      },
      "source": [
        "We import all the main important **dependencies** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoiW3DW47srm"
      },
      "outputs": [],
      "source": [
        "import random, os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import shutil\n",
        "\n",
        "\n",
        "def tf_seed(seed=0):\n",
        "  np.random.seed(seed)  # numpy seed\n",
        "  tf.random.set_seed(seed)  # tensorflow seed\n",
        "  random.seed(seed)  # random seed\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
        "  os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  \n",
        "\n",
        "\n",
        "tf_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UxygEbcUYe6"
      },
      "source": [
        "The main idea is to classify a **bonafide or attack** human face with a fast **CNN**, setting on the top of the network an **SSD** model, pretrained on a **ResNet**, that can extract faces from images.\n",
        "\n",
        "The network will have two main convolutional steps, in which we are going to extract deep features from the second phase. It uses in sequence **Conv2d + BatchNormalization** to learn faster, normalizing batch feature maps to avoid **overfitting**. However, this is why is important to set a right batch size and training data to avoid the opposite problem, then underfitting. The BatchNormalization, since this is a shallow network, won't have the so called vanishing gradient problem during the first training epochs.\n",
        "\n",
        "There are also, at the end of each phase, a Dropout layer. This is because it's important to learn as much details as possible to distinguish a \"real or fake\" person in front of a camera. So switching off weights during training randomly, will have the consequence to distribute better the learning of important features among all the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e23Y0eD3K0Q7"
      },
      "source": [
        "# DEFINE NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8JXHbf1NAkl"
      },
      "source": [
        "## Originary Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PARxPl2btms"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(input)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  #to avoid overfitting \n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekUqG6Q_NI4W"
      },
      "source": [
        "## AttackNet v2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT_0AQV2Hk0O"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  y = tf.keras.layers.Conv2D(16, 3, padding = 'same')(input)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "  z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.add([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  y = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.add([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(128, activation = 'tanh')(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF972weGK-zC"
      },
      "source": [
        "# MSSPOOF DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqP77ScjVv-q"
      },
      "source": [
        "We define absolute paths that will be used.\n",
        "\n",
        "The dataset is the following: https://www.idiap.ch/en/dataset/msspoof\n",
        "\n",
        "I. Chingovska, N. Erdogmus, A. Anjos. S. Marcel, “Face Recognition Systems Under Spoofing Attacks”, in Springer “Face Recognition Across the Imaging Spectrum” (Editor Thirimachos Bourlai), 2016.\n",
        "10.1007/978-3-319-28501-6_8\n",
        "https://publications.idiap.ch/index.php/publications/show/3539\n",
        "\n",
        "<br><br>\n",
        "Multispectral-Spoof contains face images and printed spoofing attacks recorded in Visible (VIS) and Near-Infrared (NIR) spectra for 21 identities.\n",
        "\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "We get a subset of the original dataset, so in our case is organized in two subdirectories: **attack**, **real**. The two directories **attack** and **real** contain images for attacks and bona fide presentations, respectively, both with VIS and NIR acquisitions.\n",
        "\n",
        "## Training and Validation Data\n",
        "\n",
        "In **real** and **attack** folders, we find the **NIR** and **VIS** folders, and for each of them we have **training** and **testing** folders. We include in the training also images in **dev** folder.\n",
        "Nothing included in the **testing** folder will be seen during training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC86JJ3fNwfk"
      },
      "source": [
        "## Define Root Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZXrDHVfVAOR"
      },
      "outputs": [],
      "source": [
        "root_folder=\"/content/drive/MyDrive/liveness_project/\"\n",
        "\n",
        "\n",
        "protoPath=os.path.join(root_folder,'face_detector/deploy.prototxt.txt')\n",
        "modelPath = os.path.join(root_folder,\n",
        "\t\"face_detector/res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "\n",
        "face_confidence=0.68\n",
        "\n",
        "MSSpoof_extracted=os.path.join(root_folder,'dataset/training/MSSpoof/images/')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#we define where to save output detected faces and the corresponding depth\n",
        "output_attack_training_folder=os.path.join(root_folder,'dataset/training/MSSpoof_dataset/images/attack_training/')\n",
        "output_attack_validation_folder=os.path.join(root_folder,'dataset/training/MSSpoof_dataset/images/attack_validation/')\n",
        "\n",
        "output_bonifade_training_folder=os.path.join(root_folder,'dataset/training/MSSpoof_dataset/images/bonifade_training/')\n",
        "output_bonifade_validation_folder=os.path.join(root_folder,'dataset/training/MSSpoof_dataset/images/bonifade_validation/')\n",
        "\n",
        "\n",
        "save_augmented_images =  os.path.join(root_folder,'dataset/training/MSSpoof/augmented_images/') #output of online augmented images \n",
        "save_model_h5 = os.path.join(root_folder,'model_MSSpoof.h5') \n",
        "save_labels = os.path.join(root_folder,'model_MSSpoof_labels')\n",
        "save_training_metrics_plot = os.path.join(root_folder,'MSSpoof_plot.png')\n",
        "\n",
        "output_np_training_array = os.path.join(root_folder,'MSSpoof_training_array')\n",
        "\n",
        "\n",
        "\n",
        "def get_folders(image_type, training):\n",
        "\tinit_folders=[]\n",
        "\n",
        "\tnew_root=os.path.join(MSSpoof_extracted,image_type) #ex. images/attack/\n",
        "\n",
        "\n",
        "\tinit_folders.append(os.path.join(new_root, \"VS\")) #ex. images/attack/VS\n",
        "\tinit_folders.append(os.path.join(new_root, \"IR\")) #ex. images/attack/IR\n",
        "\n",
        "\tfinal_folders=[]\n",
        "\t \n",
        "\tif image_type==\"attack\": \n",
        "\n",
        "\t\tfor folder in init_folders:\n",
        "\t\t\t#ex. images/attack/VS\n",
        "\t\t\tif training:\n",
        "\t\t\t\tfinal_folders.append(os.path.join(folder, \"VS\",\"train\")) #ex. images/attack/VS/VS/train\n",
        "\t\t\t\tfinal_folders.append(os.path.join(folder, \"VS\",\"dev\"))\n",
        "\t\t\t\tfinal_folders.append(os.path.join(folder, \"IR\",\"train\"))\n",
        "\t\t\t\tfinal_folders.append(os.path.join(folder, \"IR\",\"dev\"))\n",
        "\t\t\telse:\n",
        "\t\t\t\tfinal_folders.append(os.path.join(folder, \"VS\",\"test\")) #ex. images/attack/VS/VS/test\n",
        "\t\t\t\tfinal_folders.append(os.path.join(folder, \"IR\",\"test\"))\n",
        "\n",
        "\t  \t#result for ex. \timages/attack/VS/VS/train,images/attack/VS/VS/dev,images/attack/VS/IR/train,images/attack/VS/IR/dev\n",
        "\n",
        "\t\n",
        "\telse:\n",
        "\t\t for folder in init_folders:\n",
        "\t\t\t#ex. images/real/VS\n",
        "\t\t\t if training:\n",
        "\t\t\t\t  final_folders.append(os.path.join(folder, \"train\")) #ex. images/real/VS/train\n",
        "\t\t\t\t  final_folders.append(os.path.join(folder, \"dev\"))\n",
        "\t\t\t else:\n",
        "\t\t\t\t  final_folders.append(os.path.join(folder, \"test\")) #ex. images/real/VS/test\n",
        "\n",
        "\t  \t#result for ex. \timages/attack/VS/train,images/attack/VS/dev,images/attack/VS/train\n",
        "\n",
        "\treturn final_folders\n",
        "\n",
        "\n",
        "attack_folders_training = get_folders(\"attack\",training=True)\n",
        "attack_folders_validation = get_folders(\"attack\",training=False)\n",
        "\n",
        "bonifade_folders_training = get_folders(\"real\",training=True)\n",
        "bonifade_folders_validation = get_folders(\"real\",training=True)\n",
        "\n",
        "training_directories=[attack_folders_training, attack_folders_validation, bonifade_folders_training, bonifade_folders_validation]\n",
        "output_directories=[output_attack_training_folder, output_attack_validation_folder, output_bonifade_training_folder, output_bonifade_validation_folder]\n",
        "\n",
        "\n",
        "#create directories where to save images if they don't exist\n",
        "for output_dir in output_directories:\n",
        "\tif not os.path.exists(output_dir): \n",
        "\t  os.makedirs(output_dir)\n",
        "\t  print(\"created \",  output_dir)\n",
        "\t \n",
        "\t \n",
        "print(\"Attack folders for training: \" , attack_folders_training)\n",
        "print(\"Attack folders for validation: \" , attack_folders_validation)\n",
        "print(\"Bonifade folders for training: \" , bonifade_folders_training)\n",
        "print(\"Bonifade folders for validation: \" , bonifade_folders_validation)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HE6q3yKLb16"
      },
      "source": [
        "# NETWORK PARAMETERS FOR TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La8H1nBNILps"
      },
      "outputs": [],
      "source": [
        "final_x = 32\n",
        "final_y = 32\n",
        "\n",
        "n_channels = 3\n",
        "\n",
        "dim = (final_x, final_y)\n",
        "\n",
        "epochs=30\n",
        "batch_size=16\n",
        "init_learning_rate= 1e-4\n",
        "\n",
        "#return normalized image\n",
        "def resize_normalize_image(image):\n",
        "  image=cv2.resize(image, dim)\n",
        "  return (image)/np.max(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzeH6X9BLKLe"
      },
      "source": [
        "# FACE PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjPSQ7cRG9-F"
      },
      "source": [
        "## Network to Detect Faces\n",
        "\n",
        "We define the SSD network that detects face inside an image.\n",
        "\n",
        "In this case, since we want to save in .pgm format, we convert image to grayscale before, otherwise `cv2.imwrite` won't work correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNYKW76cU8KY"
      },
      "outputs": [],
      "source": [
        "\n",
        "#read ssd model to detect faces\n",
        "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath) \n",
        "\n",
        "\n",
        "# detect the most probable face, or all of detected face\n",
        "# return coordinates of bounding box for each face and the face itself as a cv2 object\n",
        "def detect_save_face(frame, output_path=None, multiple_output=False):\n",
        "\n",
        "    (h, w) = frame.shape[:2]\n",
        "\n",
        "    # we preprocess image normalizing each r-g-b channel subtracting the values in the last tuple\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
        "                                 (300, 300), (104.0, 177.0, 123.0))\n",
        "    # pass the blob through the network and obtain the detections and\n",
        "    # predictions\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()   #output => (1, 1, 20, [1, 0.9, .... ]   )\n",
        "    faces = []\n",
        "    coordinates=[]\n",
        "    \n",
        "    # ensure at least one face was found\n",
        "    if len(detections) > 0:\n",
        "\n",
        "       \n",
        "        \n",
        "        max_i=np.argmax(detections[0, 0, :, 2]) #this is the most probable detected face\n",
        "\n",
        "        #if we want to output the most probable detected face or all the faces\n",
        "        min_range = 0 if multiple_output==True else max_i \n",
        "        max_range = detections.shape[2] if multiple_output==True else max_i + 1\n",
        "\n",
        "        for i in range(min_range, max_range):\n",
        "\n",
        "          # we get the model confidence\n",
        "          confidence = detections[0, 0, i, 2]\n",
        "\n",
        "          if confidence >= face_confidence:\n",
        "            # compute the (x, y)-coordinates of the bounding box for\n",
        "            # the face and extract the face ROI\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "            # we save coordinates \n",
        "            coordinates.append(box.astype(\"int\"))\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "\n",
        "            #we save the face\n",
        "            faces.append(face)\n",
        "            \n",
        "\n",
        "            try:\n",
        "            \n",
        "              #we save the image if output_path is defined\n",
        "              if output_path is not None:\n",
        "                gray = cv2.cvtColor(face, cv2.COLOR_BGR2GRAY)\n",
        "                cv2.imwrite(output_path, gray)\n",
        "             \n",
        "            except:\n",
        "              print(\"error\")\n",
        "              pass\n",
        "\n",
        "    return faces, coordinates\n",
        "\n",
        "#frame_mask_output = [mask_genuine_folder, mask_fake_folder]\n",
        "\n",
        "#return file name given a path\n",
        "def get_file_name(path):\n",
        "    base = os.path.basename(path)\n",
        "    return os.path.splitext(base)[0]\n",
        "\n",
        "#return file extension given a path\n",
        "def get_file_type(path): \n",
        "    base = os.path.basename(path)\n",
        "    return os.path.splitext(base)[1] # ex. .png, .jpg\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z2RBn3fImps"
      },
      "source": [
        "## Extract Faces from Images\n",
        "\n",
        "We are going to take videos or images from our dataset. For each frame will be detected **one** face, and this one will be saved to the output folders defined before. The code reserves the possibility for the user to define also a mask for each image. In this case, the detected face will be also \"cropped\" from the mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QPqFZrOtse2"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_images_per_folder=2000000 #we want to take a maximum number of images for training since limited hardware resources\n",
        "\n",
        "\n",
        "#for each training input directory, search for color image\n",
        "#for each color image (RGB), we take the corresponding align_to_depth and depth images \n",
        "#for each image and depth, we detect one face\n",
        "def take_images():\n",
        "    directory_counter = 0\n",
        "    file_counter = 0\n",
        "\n",
        "    for main_list in training_directories:\n",
        "\n",
        "          folder_file_counter=0\n",
        "\n",
        "          d_index=0\n",
        "\n",
        "          print(\"we're on array %d/%d\" % (directory_counter + 1, len(training_directories)))\n",
        "\n",
        "\n",
        "          for directory in main_list:\n",
        "\n",
        "            d_index+=1\n",
        "\n",
        "            print(\"we're on %s\" % directory)\n",
        "\n",
        "          # files = [file for file in os.listdir(directory[0])]\n",
        "\n",
        "            list_images = [file for file in os.listdir(directory)]\n",
        "\n",
        "            #we shuffle training data since we just take a limited number of images\n",
        "            random.Random(20).shuffle(list_images)\n",
        "\n",
        "            print(\"found %d images for folder n. %d/%d\" % (len(list_images), d_index, len(main_list)))\n",
        "\n",
        "            for file in list_images:\n",
        "\n",
        "                file_path=os.path.join(directory, file)\n",
        "\n",
        "                if folder_file_counter > max_images_per_folder:\n",
        "                  break\n",
        "\n",
        "              #  file_path = os.path.join(directory, file)\n",
        "                \n",
        "                color=cv2.imread(file_path)\n",
        "                              \n",
        "                detect_save_face(color,  (os.path.join(output_directories[directory_counter], \"%s%s\" % (\n",
        "                        \"frame_%d\" % file_counter, get_file_type(file_path)))))\n",
        "                    \n",
        "                folder_file_counter+=1\n",
        "\n",
        "                print(\"save face detected on image n. %d, for folder n. %d\" % (file_counter, directory_counter + 1))\n",
        "\n",
        "\n",
        "                file_counter += 1\n",
        "\n",
        "          directory_counter += 1\n",
        "\n",
        "\n",
        "take_images() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfcLzIHfNhAE"
      },
      "source": [
        "\n",
        "# TRAINING SECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tJoTeyWXjw"
      },
      "source": [
        "## Get Training and Validation Data\n",
        "\n",
        "For each directory where we saved detected faces, we are going to create two arrays: images, that will be concatenated with the depth mask, **by default**, and labels (0 for bonafide, 1 for attackers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izwSR4HW0zbE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_images(directories, balance_dataset=True, undersampling=False):\n",
        "\n",
        "    image_list = []\n",
        "    label_list = []\n",
        "\n",
        "    classes = 0\n",
        "\n",
        "    images_per_class=[]\n",
        "\n",
        "    tf_seed()\n",
        "\n",
        "    #for each input directory\n",
        "    for i in directories:\n",
        "\n",
        "        list_images = [f for f in os.listdir(i) if os.path.isfile(os.path.join(i, f))]\n",
        "\n",
        "        images_number = len(list_images)\n",
        "\n",
        "        random.Random(20).shuffle(list_images)\n",
        "\n",
        "\n",
        "\n",
        "        print(\"n %d\" % images_number)\n",
        "\n",
        "        #if we use undersampling\n",
        "        if undersampling:\n",
        "          images_number = images_number if ((len(images_per_class) == 0) or (images_number <= images_per_class[-1]))  else images_per_class[-1]\n",
        "        \n",
        "        \n",
        "        # to fill tensors we inizialise them\n",
        "        X = np.empty((images_number, final_x, final_y, n_channels))\n",
        "\n",
        "        L = np.empty((images_number))\n",
        "\n",
        "        ipp = 0\n",
        "\n",
        "        for im_name in list_images:\n",
        "            \n",
        "            im=os.path.join(i, im_name)\n",
        "\n",
        "\n",
        "            # to get \"balanced dataset\" using undersampling method\n",
        "            if (balance_dataset and undersampling) and len(images_per_class)  and ipp == images_per_class[-1]:\n",
        "                break\n",
        "\n",
        "            image = cv2.imread(im)\n",
        "            image=resize_normalize_image(image)\n",
        "\n",
        "          \n",
        "                \n",
        "\n",
        "            X[ipp, ..., :3] = image # each original image is rgb\n",
        "            L[ipp] = classes  # 0 for real, 1 for fake for binary classification\n",
        "\n",
        "            ipp += 1\n",
        "        \n",
        "        # check if the current class has lower images than the before\n",
        "        # in this case we undersample the majority class randomly removing elements\n",
        "        # we will have a balanced dataset: same images for all the classes\n",
        "\n",
        "        if balance_dataset and undersampling:\n",
        "            if len(images_per_class) and ipp < images_per_class[-1]:\n",
        "            \n",
        "              left_shift = 0\n",
        "\n",
        "              for i in range(0, len(images_per_class)):\n",
        "\n",
        "                  diff = images_per_class[i] - ipp\n",
        "\n",
        "                  for j in range(0, diff):\n",
        "                      random_index = np.random.randint(left_shift, images_per_class[i] - j)\n",
        "                      image_list.pop(random_index)\n",
        "                      label_list.pop(random_index)\n",
        "                    #  print(random_index)\n",
        "\n",
        "                  left_shift += images_per_class[i] - diff\n",
        "\n",
        "        if undersampling or balance_dataset==False:\n",
        "              for image in X:\n",
        "                image_list.append(image)\n",
        "\n",
        "              for label in L:\n",
        "                label_list.append(label)\n",
        "        else:\n",
        "              image_array=[]\n",
        "              label_array=[]\n",
        "            \n",
        "              for image in X:\n",
        "                image_array.append(image)\n",
        "\n",
        "              image_list.append(np.array(image_array, dtype=\"float\"))\n",
        "\n",
        "             \n",
        "              for label in L:\n",
        "                label_array.append(label)\n",
        "\n",
        "              label_list.append(label_array)\n",
        "\n",
        "              #[[2k], [1k]]\n",
        "        \n",
        "        if undersampling or balance_dataset==False:\n",
        "            images_per_class.append(ipp)\n",
        "        else:\n",
        "            images_per_class.append(images_number)\n",
        "\n",
        "        classes += 1\n",
        "\n",
        "    #offline augmentation\n",
        "    if balance_dataset and undersampling==False:\n",
        "\n",
        "      \n",
        "         max=np.max((images_per_class)) #[4k,3k,2k,1k]\n",
        "\n",
        "         for j in range(0, len(images_per_class)):  #[[4k], [4k], [4k], [4k]]\n",
        "            if images_per_class[j] < max:\n",
        "              diff=max - images_per_class[j] #how many images to generate\n",
        "\n",
        "              image_array=image_list[j]   #[3k]\n",
        "              label_array=label_list[j]   #[3k]\n",
        "\n",
        "              new_image_array=[]\n",
        "             \n",
        "\n",
        "              #use ImageDataGenerator \n",
        "              offline_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                       \twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\t                       horizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "              offline_generator=offline_generator.flow(image_array, batch_size=1, seed = 2)\n",
        "\n",
        "              #generate the reamining images respect the majority class, to balance the dataset\n",
        "              generated_images=0\n",
        "              while generated_images < diff:\n",
        "                     \n",
        "                  for i in range(0,len(offline_generator)):\n",
        "                        if generated_images == diff:\n",
        "                          break\n",
        "                        batch= next(offline_generator)\n",
        "                        # print(batch.shape)\n",
        "                        #print(new_image_array.shape)\n",
        "                        new_image_array.append(batch[0])\n",
        "                        \n",
        "                        label_array.append(j)\n",
        "\n",
        "                      \n",
        "                        generated_images += 1\n",
        "\n",
        "            \n",
        "     \n",
        "              image_list[j]=np.append(image_list[j], new_image_array, axis=0)\n",
        "\n",
        "\n",
        "         # image_list and label_list were of type [[...], [...]] for attack and bonafide classes\n",
        "         # we need a final array of type [image1,image2,...] for images and [0,1,...] for labels\n",
        "\n",
        "         new_image_list= np.empty((0, final_x, final_y, n_channels))\n",
        "         new_label_list=[]\n",
        "\n",
        "\n",
        "         for class_value_array in image_list:\n",
        "           new_image_list = np.vstack((new_image_list,class_value_array))\n",
        "\n",
        "         for class_value_array in label_list:\n",
        "           new_label_list += class_value_array\n",
        "\n",
        "         image_list = new_image_list\n",
        "         label_list = new_label_list\n",
        "\n",
        "         print(image_list.shape)\n",
        "         print(len(label_list))\n",
        "         \n",
        "        # im_show(new_image_list[2]) # we can show up an image \n",
        "      \n",
        "        \n",
        "    #create the one-hot encoded vector: [0 1], [1 0]\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(label_list)\n",
        "    labels = tf.keras.utils.to_categorical(labels, 2)\n",
        "\n",
        "    if type(image_list) is np.ndarray:\n",
        "      return image_list, labels, le\n",
        "\n",
        "    return  np.array(image_list, dtype=\"float\"), labels, le\n",
        "\n",
        "\n",
        "\n",
        "read_from_np_array = True\n",
        "\n",
        "\n",
        "\n",
        "## WE CAN SAVE NUMPY RESULTS AND REUSE THEM LATER ##\n",
        "\n",
        "if read_from_np_array:\n",
        "      (trainX, testX, trainY, testY) = pickle.loads(open(output_np_training_array, \"rb\").read())\n",
        "      le = pickle.loads(open(save_labels, \"rb\").read())\n",
        "\n",
        "## WE ARE GOING TO GET THE TRAIN AND VALIDATION DATASETS, AND WE SAVE THE NUMPY ARRAYS ##\n",
        "else:\n",
        "      \n",
        "      trainX, trainY, label_encoder = get_images([output_bonifade_training_folder, output_attack_training_folder], undersampling=True)\n",
        "\n",
        "     \n",
        "      #will be ignored if we set split_randomly_training_and_validation variable to True\n",
        "      testX, testY, _ = get_images([output_bonifade_validation_folder, output_attack_validation_folder], undersampling=True)\n",
        "\n",
        "\n",
        "      #save model labels \n",
        "      f = open(save_labels, \"wb\")\n",
        "      f.write(pickle.dumps(label_encoder))\n",
        "      f.close()\n",
        "\n",
        "      #save results\n",
        "      pickle.dump((trainX, testX, trainY, testY), open(output_np_training_array, 'wb'))\n",
        "\n",
        "\n",
        " \n",
        "print(\"Shape training: \", trainX.shape)\n",
        "print(\"Shape validation: \", testX.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_lCe8-YMFIb"
      },
      "source": [
        "## Augmentation Online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vz29KTXlJJIi",
        "outputId": "42548f67-bb0b-42cf-fb97-3caa06b5a9bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenght training batches  103\n",
            "lenght training data  1638\n",
            "lenght validation data  1638\n"
          ]
        }
      ],
      "source": [
        "augmentation_online=True\n",
        "\n",
        "\n",
        "\n",
        "#the random seed for online augmentation\n",
        "generator_seed=10\n",
        "\n",
        " \n",
        "if augmentation_online:\n",
        "\t\t#online augmentation of training images\n",
        "\t\taug_training = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "\t\t \t    width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15,\n",
        "\t\t\t horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "\t\t#aug_training = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "\n",
        "\t\t# output an endless Iterator of batches\n",
        "\t\t# shuffle images, so the first generated batch is from batch_size random training images\n",
        "\n",
        "\t\ttrain_generator=aug_training.flow(trainX, trainY, batch_size=batch_size, shuffle=True, seed=generator_seed)\n",
        "\n",
        "\n",
        "\t\t# will be equal to np.ceil(len(train(X))/batch_size)\n",
        "\t\t# notice: if len(trainX) / batch_size is a float value, the last batch will contain less images than batch_size value\n",
        "\t\tprint(\"lenght training batches \", len(train_generator))\n",
        "\t\tprint(\"lenght training data \", len(trainX))\n",
        "\t\tprint(\"lenght validation data \", len(testX))\n",
        "\n",
        "\t\t# we could use also a validation_generator, without augment images\n",
        "\t\t# this \"trick\" could be done for set up a valdation batch size on model.fit().. then at the end of an epoch, each batch will be validated\n",
        "\n",
        "\t\t#aug_validation=ImageDataGenerator()\n",
        "\t\t#validation_generator=aug_validation.flow(testX, testY, batch_size=batch_size, shuffle=False, seed=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp8mvCvwLH5N"
      },
      "source": [
        "We can output the generator images organized in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE71Q14r1oRq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# we generate batch_size images, beginning from the first batch\n",
        "batchx, batchy = next(train_generator)\n",
        "for i in range(batchx.shape[0]):\n",
        "    fig, (ax1) = plt.subplots(1) \n",
        "    ax1.set_title('image')  \n",
        "    ax1.imshow(batchx[i][:,:,0], cmap='gray')\n",
        "    print(\"Label-%d:\"%i, np.argmax(batchy[i]))\n",
        "\n",
        "\n",
        "train_generator.reset() #once an epoch ends, this function will be called to reset data\n",
        "\n",
        "# we generate exactly len(trainX) - batch_size images beginning from the first batch, since we have reset the train_generator on the line before\n",
        "for i in range(0,len(train_generator) - 1):\n",
        " break # remove to see all training augmented images for an epoch\n",
        " batchx, batchy = next(train_generator)\n",
        " for i in range(batchx.shape[0]):\n",
        "    fig, (ax1) = plt.subplots(1) \n",
        "    ax1.set_title('image')  \n",
        "    ax1.imshow(batchx[i][:,:,0], cmap='gray')\n",
        "    print(\"Label-%d:\"%i, batchy[i][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OFFLINE AUGMENTATION (optional)"
      ],
      "metadata": {
        "id": "rtE7tE8ezNw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each image in the training set, we're going to augment the image itself and we add it to the training. So if we have 2k images for the training, the final length of the dataset will be 4k."
      ],
      "metadata": {
        "id": "GVBP6_3baHSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "augmentation_online=False\n",
        "\n",
        "generator_seed=10\n",
        "     \n",
        "\n",
        "if not augmentation_online:\n",
        "\n",
        "\n",
        "  to_augment=len(trainY) \n",
        "\n",
        "  #print(to_augment)\n",
        "  # to fill tensors we inizialise them\n",
        "  X = np.empty((to_augment, final_x, final_y, n_channels), dtype=\"float\")\n",
        "\n",
        "  L = np.empty((to_augment), dtype=\"int\")\n",
        "\n",
        "  #use ImageDataGenerator \n",
        "  offline_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "\t\t \t    width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15,\n",
        "\t\t\t horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "\n",
        "  offline_generator=offline_generator.flow(trainX, trainY, shuffle=True, batch_size=1, seed=generator_seed)\n",
        "\n",
        "  #for all initial images\n",
        "  for i in range(0,to_augment):\n",
        "            batchx, batchy = next(offline_generator)\n",
        "            X[i] = batchx[0]\n",
        "            L[i] = np.argmax(batchy[0])\n",
        "          \n",
        "  \n",
        "  labels = le.fit_transform(L)\n",
        "  labels = tf.keras.utils.to_categorical(labels, 2)\n",
        "\n",
        "  trainX = np.vstack((trainX, X))\n",
        "  trainY = np.vstack((trainY, labels))\n",
        "\n",
        "\n",
        "print(\"Shape training: \", trainX.shape)\n",
        "print(\"Shape validation: \", testX.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "HvXvTIxcXnOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu2_1UwcLVuF"
      },
      "source": [
        "## **Train the Model**\n",
        "\n",
        "We train the **model** and we save in a **CustomCallback** instance the average loss and accuracy for each training and validation batch. In this way, we can create a more defined training plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV09uMO-1jj7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch, psutil\n",
        "\n",
        "\n",
        "class MemoryUsage(tf.keras.callbacks.Callback):\n",
        "\n",
        "   def __init__(self):\n",
        "      # setting device on GPU if available, else CPU\n",
        "      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      print('Using device for training:', self.device)\n",
        "      self.max_RAM=[]\n",
        "      self.max_GPU=[]\n",
        "\n",
        "   def get_memory_usage(self):\n",
        "      gpu_dict = tf.config.experimental.get_memory_info('GPU:0')\n",
        "      tf.print('\\n GPU memory details [current: {} gb, peak: {} gb]'.format(\n",
        "          float(gpu_dict['current']) / (1024 ** 3), \n",
        "          float(gpu_dict['peak']) / (1024 ** 3)))\n",
        "   \n",
        "   def get_size(self, byte, suffix=\"GB\"):\n",
        "    factor = 1024\n",
        "    \n",
        "    for unit in [\"\", \"K\", \"M\", \"GB\", \"T\", \"P\"]:\n",
        "        if byte < factor:\n",
        "            return f\"{byte:.2f} GB\"\n",
        "        byte /= factor\n",
        "\n",
        "   def on_train_end(self, epoch, logs=None):\n",
        "      i=np.argmax(self.max_RAM)\n",
        "      self.get_memory_usage()\n",
        "      print(\"MAX RAM USAGE: %s / %s (%s)\" % (self.get_size(self.max_RAM[i][0]), self.get_size(self.max_RAM[i][1]), str(self.max_RAM[i][2]) + \"%\" ))\n",
        "      \n",
        "\n",
        "\n",
        "   \n",
        "   def on_epoch_end(self,epoch,logs=None):\n",
        "      svmem = psutil.virtual_memory()\n",
        "      self.max_RAM.append((svmem.active, svmem.total, svmem.percent))\n",
        "      #self.get_memory_usage()\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.5)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.5)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate, decay=init_learning_rate/epochs)\n",
        "\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.99)\n",
        "tf_seed()\n",
        "model = LiveNet((final_y,final_x,n_channels))\n",
        "\n",
        "callback_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)\n",
        "memory_usage = MemoryUsage()\n",
        "\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])  # same of binary_crossentropy in this case\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#validation batch_size is equal to batch_size in this case (32)\n",
        "#check https://github.com/keras-team/keras/blob/v2.9.0/keras/engine/training.py#L1099-L1472\n",
        "if augmentation_online:\n",
        "    H=model.fit(\n",
        "      train_generator, \n",
        "      validation_data = (testX, testY), \n",
        "      callbacks = [memory_usage, callback_early_stopping],\n",
        "      epochs= epochs)\n",
        "else:\n",
        "   \n",
        "   H=model.fit(\n",
        "      trainX, trainY, \n",
        "      batch_size = batch_size,\n",
        "      shuffle=True,\n",
        "      validation_data = (testX, testY), \n",
        "      callbacks = [memory_usage, callback_early_stopping],\n",
        "      epochs= epochs)\n",
        "\n",
        "print(\"TRAINING TIME\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print((history.epoch_batch_train_acc[0])) #show first epoch batches averaged values\n",
        "\n",
        "print(\"[INFO] serializing network to '{}'...\".format(save_model_h5))\n",
        "model.save(save_model_h5, save_format=\"h5\")\n",
        "\n",
        "\n",
        "if not read_from_np_array:\n",
        "   #save model labels \n",
        "      f = open(save_labels, \"wb\")\n",
        "      f.write(pickle.dumps(label_encoder))\n",
        "      f.close()\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "fig=plt.figure(figsize=(12,7))\n",
        "\n",
        "\n",
        "\n",
        "new_epochs=len(H.history['loss'])\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "\n",
        "plt.savefig(save_training_metrics_plot)\n",
        "\n",
        "\n",
        "# notice accuracy at the end of each epoch is the mean all over the batches, the same if we have more than 1 validation batch\n",
        "\n",
        "#https://keras.io/guides/training_with_built_in_methods/\n",
        "#https://keras.io/getting_started/faq/#why-is-my-training-loss-much-higher-than-my-testing-loss\n",
        "#https://stackoverflow.com/questions/55097362/different-accuracy-by-fit-and-evaluate-in-keras-with-the-same-dataset\n",
        "\n",
        "# the below code can give never seen images at each validation_step; more in general we divide validation dataset in batches\n",
        "# see https://stackoverflow.com/questions/56991909/how-is-the-keras-accuracy-showed-in-progress-bar-calculated-from-which-inputs-i\n",
        "\n",
        "\n",
        "#model.fit(train_generator,\n",
        " # steps_per_epoch=len(train_generator),\n",
        "  #validation_data = validation_generator,\n",
        "\t#epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjlxdHfLMwi8"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdTpA07N4Xrl"
      },
      "outputs": [],
      "source": [
        "#confusion matrix\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import itertools\n",
        "from itertools import cycle\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
        "\n",
        "model = tf.keras.models.load_model(save_model_h5)\n",
        "\n",
        "\n",
        "y_score = model.predict(testX)\n",
        "\n",
        "y_pred = np.argmax(y_score, axis = 1)\n",
        "y_test_ = np.argmax(testY, axis = 1)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \n",
        "    def precision(index):\n",
        "      return round(cm[index][index] / cm[:, index].sum(),2)\n",
        "    \n",
        "    def recall(index):\n",
        "      return round(cm[index][index] /cm[index].sum(),2)\n",
        "    \n",
        "    def F1_score(index):\n",
        "      p=precision(index)\n",
        "      r=recall(index)\n",
        "      return round((2 * p * r)/(p + r),2)\n",
        "    \n",
        "    plt.figure(figsize=(6, 6), dpi=80)\n",
        "\n",
        "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, round(cm[i, j],2),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        \n",
        "   \n",
        "\n",
        "    #### CREATE THE PRECISION / RECALL / F1_SCORE TABLE ####\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 2 + 3 / 2.5))\n",
        "    \n",
        "    col_labels=[\"Bonafide\", \"Attacker\"]\n",
        "    row_labels=['Precision','Recall','F1 Score']\n",
        "    row_func=[precision,recall,F1_score]\n",
        "    table_vals=[]  \n",
        "\n",
        "    row_colors = np.full(len(row_labels), 'linen')\n",
        "    col_colors = np.full(len(col_labels), 'lavender')\n",
        "\n",
        "    \n",
        "    for i in range(0, len(row_labels)):\n",
        "\n",
        "      row=[]\n",
        "\n",
        "      for j in range(0, len(col_labels)):\n",
        "         row.append(row_func[i](j))\n",
        "\n",
        "      table_vals.append(row)\n",
        "    \n",
        "\n",
        "    # the rectangle is where I want to place the table\n",
        "    table = plt.table(cellText=table_vals,\n",
        "                  cellLoc='center',\n",
        "                  rowColours=row_colors,\n",
        "                  rowLabels=row_labels,\n",
        "                  rowLoc='center',\n",
        "                  colColours=col_colors,\n",
        "                  colLabels=col_labels,\n",
        "                  loc='center')\n",
        "    table.scale(1, 2)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    \n",
        "\n",
        "class_names = {\"Bonafide\", \"Attacker\"}\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_, y_pred)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion Matrix')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}