{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irqzKVH55WNX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aFsc5HPUO-u"
      },
      "source": [
        "We import all the main important **dependencies** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoiW3DW47srm"
      },
      "outputs": [],
      "source": [
        "import random, os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import shutil\n",
        "\n",
        "\n",
        "def tf_seed(seed=0):\n",
        "  np.random.seed(seed)  # numpy seed\n",
        "  tf.random.set_seed(seed)  # tensorflow seed\n",
        "  random.seed(seed)  # random seed\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
        "  os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  \n",
        "def tf2_seed(seed=0):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "  tf.keras.utils.set_random_seed(seed)\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "\n",
        "                   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tf_seed()\n",
        "\n",
        "#see https://keras.io/getting_started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ei8H-D3nPycl"
      },
      "source": [
        "# DEFINE NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UxygEbcUYe6"
      },
      "source": [
        "The main idea is to classify a **bonafide or attack** human face with a fast **CNN**, setting on the top of the network an **SSD** model, pretrained on a **ResNet**, that can extract faces from images.\n",
        "\n",
        "The network will have two main convolutional steps, in which we are going to extract deep features from the second phase. It uses in sequence **Conv2d + BatchNormalization** to learn faster, normalizing batch feature maps to avoid **overfitting**. However, this is why is important to set a right batch size and training data to avoid the opposite problem, then underfitting. The BatchNormalization, since this is a shallow network, won't have the so called vanishing gradient problem during the first training epochs.\n",
        "\n",
        "There are also, at the end of each phase, a Dropout layer. This is because it's important to learn as much details as possible to distinguish a \"real or fake\" person in front of a camera. So switching off weights during training randomly, will have the consequence to distribute better the learning of important features among all the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhtzZEVjP2mc"
      },
      "source": [
        "## Originary Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PARxPl2btms"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(input)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  #to avoid overfitting \n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeKx5XBHP9Rp"
      },
      "source": [
        "## AttackNet v1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2yCsuL5W1iC"
      },
      "source": [
        "This is an improvement of the initial CNN. Since the most important architectures in the state of the art use, to improve efficacy, skip connections (summing and concatenating layers), for example **Yolo**, **U-Net**, **ResNet**, we introduced, for each phase, another Convolutional Layer with more filters at the end. In this case the last two Conv2D won't be normalized soon, but first they're concatenated with the first layer of the phase. In this case, we make the network deeper, but at the same time, we try to improve backpropagation introducing concatenating layers that will add the information of the feature maps of the first layers to the last, thus influencing they're output, and then how the weights will be updated, improving explosion or vanishing gradient issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIbyozQ9peu_"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "  dropout_seed=2\n",
        "\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  y = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(input)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(y)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(x)\n",
        "  z = tf.keras.layers.Conv2D(64, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.concatenate([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25, seed=dropout_seed)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  y = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(y)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "  z = tf.keras.layers.Conv2D(128, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.concatenate([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25, seed=dropout_seed)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5, seed=dropout_seed)(x)\n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1QmCKqSQEiE"
      },
      "source": [
        "## AttackNet v2.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwSb1yDAxPjJ"
      },
      "source": [
        "This third architecture introduces the use of **LeakyReLU** instead of ReLU. The last layer is now activated by **tanh function**. This was done to try to improve the generalization capability of the network solving the \"sparsity\" of ReLU, that in several cases can be useful though, and improving in this way the variability of the outputs of the network. Since now we use in the last fully connected layer the tanh activation, LeakyReLU could be useful also for the vanishing gradient problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8GPZyZavBe6"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "      chan_dim=-1 #feature map channel\n",
        "\n",
        "      #first convolutional phase\n",
        "      input = tf.keras.layers.Input(input_size)\n",
        "      y = tf.keras.layers.Conv2D(16, 3, padding = 'same')(input)\n",
        "      x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "      x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "      x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "      x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "      x = tf.keras.layers.Conv2D(64, 3, padding = 'same')(x)\n",
        "      z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "      x = tf.keras.layers.concatenate([y,z])\n",
        "      x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "      x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "      x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "      #second convolutional phase\n",
        "      y = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "      x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "      x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "      x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "      x = tf.keras.layers.Conv2D(128, 3, padding = 'same')(x)\n",
        "      z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "      x = tf.keras.layers.concatenate([y,z])\n",
        "      x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "      x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "      x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "      #dense phase\n",
        "      x = tf.keras.layers.Flatten()(x)\n",
        "      x = tf.keras.layers.Dense(128, activation = 'tanh')(x)\n",
        "\n",
        "      #x = BatchNormalization()(x)\n",
        "      x = tf.keras.layers.Dropout(0.5)(x)\n",
        "      \n",
        "      #softmax\n",
        "      output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        "      return tf.keras.models.Model(input, output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AttackNet v2.2"
      ],
      "metadata": {
        "id": "Tmuw_6DNNEzd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kEgTvxpKFkT"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  y = tf.keras.layers.Conv2D(16, 3, padding = 'same')(input)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "  z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.add([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  y = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.add([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(128, activation = 'tanh')(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC86JJ3fNwfk"
      },
      "source": [
        "# DATASET\n",
        "\n",
        "In this case, we built our dataset that consist on **real** images, and **for each** real person, there is one or more **fake** versions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUrAr-vqQLoy"
      },
      "source": [
        "## Define Root Folders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqP77ScjVv-q"
      },
      "source": [
        "We define absolute paths that will be used.\n",
        "\n",
        "We will split dataset in **training**, **validation** subsets. For **testing**, we created 44 extra images for both classes, that have been specifically selected to try to \"break down\" the results of the network. \n",
        "We added those extra images to the validation dataset, but they could be used to make a quick network evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZXrDHVfVAOR"
      },
      "outputs": [],
      "source": [
        "root_folder = \"/content/drive/MyDrive/liveness_project/\"\n",
        "\n",
        "\n",
        "protoPath = os.path.join(root_folder, 'face_detector/deploy.prototxt.txt')\n",
        "modelPath = os.path.join(root_folder,\n",
        "                         \"face_detector/res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "\n",
        "face_confidence = 0.75\n",
        "\n",
        "\n",
        "path_to_videos = os.path.join(root_folder, 'dataset', 'training', 'videos')\n",
        "path_to_outputs = os.path.join(root_folder, 'dataset', 'training', 'images')\n",
        "path_to_test_input = os.path.join(\n",
        "    root_folder, 'dataset', 'training', 'final_test')\n",
        "\n",
        "\n",
        "################### INPUT FOLDERS ###################\n",
        "\n",
        "input_training_real_folder = os.path.join(\n",
        "    path_to_videos, 'real', 'training')  # input of real training videos\n",
        "input_training_fake_folder = os.path.join(\n",
        "    path_to_videos, 'fake', 'training')  # input of real training videos\n",
        "\n",
        "input_validation_real_folder = os.path.join(\n",
        "    path_to_videos, 'real', 'validation')  # input for real validation videos\n",
        "input_validation_fake_folder = os.path.join(\n",
        "    path_to_videos, 'fake', 'validation')  # input for fake validation videos\n",
        "\n",
        "input_test_real_folder = os.path.join(\n",
        "    path_to_test_input, 'real')  # EXTRA (hard) input for real testing videos\n",
        "input_test_fake_folder = os.path.join(\n",
        "    path_to_test_input, 'fake')  # EXTRA (hard) input for fake testing videos\n",
        "\n",
        "################### OUTPUT FOLDERS ###################\n",
        "\n",
        "# output of detected real faces for training\n",
        "output_training_real_folder = os.path.join(path_to_outputs, 'real', 'training')\n",
        "# output of detected fake faces for training\n",
        "output_training_fake_folder = os.path.join(path_to_outputs, 'fake', 'training')\n",
        "\n",
        "# output of detected real faces for validation\n",
        "output_validation_real_folder = os.path.join(\n",
        "    path_to_outputs, 'real', 'validation')\n",
        "# output of detected fake faces for validation\n",
        "output_validation_fake_folder = os.path.join(\n",
        "    path_to_outputs, 'fake', 'validation')\n",
        "\n",
        "# output of EXTRA (hard) detected real faces for validation\n",
        "output_test_real_folder = os.path.join(path_to_outputs, 'test', 'real')\n",
        "# EXTRA OPTIONAL output of detected fake faces for validation\n",
        "output_test_fake_folder = os.path.join(path_to_outputs, 'test', 'fake')\n",
        "\n",
        "# output of EXTRA (hard) detected real faces for validation\n",
        "save_augmented_images = os.path.join(root_folder, 'dataset/augmented_images/')\n",
        "\n",
        "\n",
        "save_model_h5 = os.path.join(root_folder, 'first_task_model.h5')\n",
        "save_labels = os.path.join(root_folder, 'first_task_model_labels')\n",
        "save_training_metrics_plot = os.path.join(\n",
        "    root_folder, 'first_task_training_plot.png')\n",
        "\n",
        "##### if save and reuse a np array for training/validation/testing images #####\n",
        "output_np_training_array = os.path.join(root_folder, \"training.array\")\n",
        "\n",
        "image_for_test = os.path.join(root_folder, 'test.jpg')\n",
        "\n",
        "# if True will delete all previous extracted faces\n",
        "delete_training_data_and_reset = False\n",
        "\n",
        "split_randomly_training_and_validation = False\n",
        "\n",
        "\n",
        "# will split entire dataset randomly for training and testing\n",
        "if delete_training_data_and_reset:\n",
        "\n",
        "    print(\"we're going to take entire dataset, then we split randomly in training and validation subsets\")\n",
        "\n",
        "    input_training_real_folder += input_validation_real_folder\n",
        "    input_training_fake_folder += input_validation_fake_folder\n",
        "\n",
        "    input_validation_real_folder = []\n",
        "    input_validation_fake_folder = []\n",
        "\n",
        "    training_directories = [\n",
        "        input_training_real_folder, input_training_fake_folder]\n",
        "    output_directories = [output_training_real_folder,\n",
        "                          output_training_fake_folder]\n",
        "\n",
        "else:\n",
        "\n",
        "    print(\"we're going to use training and validation folders\")\n",
        "\n",
        "    training_directories = [input_training_real_folder, input_validation_real_folder,\n",
        "                            input_training_fake_folder, input_validation_fake_folder]\n",
        "    output_directories = [output_training_real_folder, output_validation_real_folder,\n",
        "                          output_training_fake_folder, output_validation_fake_folder]\n",
        "\n",
        "#adding testing folders \n",
        "\n",
        "training_directories += [input_test_real_folder, input_test_fake_folder]\n",
        "output_directories += [output_test_real_folder, output_test_fake_folder]\n",
        "\n",
        "# create directories where to save images if they don't exist\n",
        "for output_dir in output_directories:\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "        print(\"created %s\", output_dir)\n",
        "\n",
        "    else:\n",
        "        if delete_training_data_and_reset:\n",
        "            shutil.rmtree(output_dir)\n",
        "            os.makedirs(output_dir)\n",
        "            print(\"recreated %s\", output_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HE6q3yKLb16"
      },
      "source": [
        "# NETWORK PARAMETERS FOR TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La8H1nBNILps"
      },
      "outputs": [],
      "source": [
        "final_x = 32\n",
        "final_y = 32\n",
        "\n",
        "n_channels = 3 #rgb\n",
        "\n",
        "dim = (final_x, final_y)\n",
        "\n",
        "epochs=25\n",
        "batch_size=32\n",
        "init_learning_rate= 1e-6\n",
        "\n",
        "#return normalized image\n",
        "def resize_normalize_image(image, max_value = 255):\n",
        "\n",
        "    image=cv2.resize(image, dim)\n",
        "    return  (image)/ np.max(image)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE1y913SQjTw"
      },
      "source": [
        "# FACE PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjPSQ7cRG9-F"
      },
      "source": [
        "## Network to Detect Faces\n",
        "\n",
        "We define the SSD network that detects face inside an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNYKW76cU8KY"
      },
      "outputs": [],
      "source": [
        "\n",
        "#read ssd model to detect faces\n",
        "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath) \n",
        "\n",
        "\n",
        "# detect the most probable face, or all of detected face\n",
        "# return coordinates of bounding box for each face and the face itself as a cv2 object\n",
        "def detect_save_face(frame, output_path=None, multiple_output=False):\n",
        "\n",
        "    (h, w) = frame.shape[:2]\n",
        "\n",
        "    # we preprocess image normalizing each r-g-b channel subtracting the values in the last tuple\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
        "                                 (300, 300), (104.0, 177.0, 123.0))\n",
        "    # pass the blob through the network and obtain the detections and\n",
        "    # predictions\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()   #output => (1, 1, 20, [1, 0.9, .... ]   )\n",
        "    faces = []\n",
        "    coordinates=[]\n",
        "\n",
        "    # ensure at least one face was found\n",
        "    if len(detections) > 0:\n",
        "        \n",
        "        max_i=np.argmax(detections[0, 0, :, 2]) #this is the most probable detected face\n",
        "\n",
        "        #if we want to output the most probable detected face or all the faces\n",
        "        min_range = 0 if multiple_output==True else max_i \n",
        "        max_range = detections.shape[2] if multiple_output==True else max_i + 1\n",
        "\n",
        "        for i in range(min_range, max_range):\n",
        "\n",
        "          # we get the model confidence\n",
        "          confidence = detections[0, 0, i, 2]\n",
        "\n",
        "          if confidence >= face_confidence:\n",
        "            # compute the (x, y)-coordinates of the bounding box for\n",
        "            # the face and extract the face ROI\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "            # we save coordinates \n",
        "            coordinates.append(box.astype(\"int\"))\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "\n",
        "            #we save the face\n",
        "            faces.append(face)\n",
        "            \n",
        "\n",
        "            try:\n",
        "              #if we had defined a mask, we take the face also in that mask\n",
        "           #   if mask is not None:\n",
        "            #    face_mask = mask[startY:endY, startX:endX]\n",
        "             #   cv2.imwrite(output_path_mask, face_mask)\n",
        "            # write the frame to disk\n",
        "              \n",
        "              #we save the image if output_path is defined\n",
        "              if output_path is not None:\n",
        "                cv2.imwrite(output_path, face)\n",
        "             \n",
        "            except:\n",
        "              pass\n",
        "\n",
        "    return faces, coordinates\n",
        "\n",
        "#frame_mask_output = [mask_genuine_folder, mask_fake_folder]\n",
        "\n",
        "#return file name given a path\n",
        "def get_file_name(path):\n",
        "    base = os.path.basename(path)\n",
        "    return os.path.splitext(base)[0]\n",
        "\n",
        "#return file extension given a path\n",
        "def get_file_type(path): \n",
        "    base = os.path.basename(path)\n",
        "    return os.path.splitext(base)[1] # ex. .png, .jpg\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8dcUn8cQqJ-"
      },
      "source": [
        "## Extract Faces from Images\n",
        "\n",
        "We are going to take videos or images from our dataset. For each frame will be detected **one** face, and this one will be saved to the output folders defined before. The code reserves the possibility for the user to define also a mask for each image. In this case, the detected face will be also \"cropped\" from the mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QPqFZrOtse2"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_images_per_class=15000 #we want to take a maximum number of images for training since limited hardware resources\n",
        "max_images_per_video=200 #to avoid to take too many similar frames per video\n",
        "shuffle_seed=20\n",
        "\n",
        "#def get_mask_name(mask_suffix, filename):\n",
        " # if mask_suffix != \"same_name\":\n",
        "  #  return get_file_name(filename) + mask_suffix\n",
        "  \n",
        "  #return filename\n",
        "\n",
        "#def get_mask_file_type(mask_suffix, filename):\n",
        " # if mask_suffix != \"same_name\":\n",
        "  #  return get_file_type(mask_suffix)\n",
        "  \n",
        "  #return get_file_type(filename)\n",
        "\n",
        "#for each training input directory, search for videos and images\n",
        "#for each video we extract a limited number of frames per second\n",
        "#for each image, we detect one face\n",
        "def take_images(mask=False, mask_suffix=\"same_name\"):\n",
        "    directory_counter = 0\n",
        "\n",
        "\n",
        "    file_counter = 0\n",
        "\n",
        "    for directory in training_directories:\n",
        "\n",
        "        class_file_counter=0\n",
        "\n",
        "        files = [file for file in os.listdir(directory)]\n",
        "\n",
        "        #we shuffle training data since we just take a limited number of images\n",
        "        random.Random(20).shuffle(files)\n",
        "\n",
        "        for file in files:\n",
        "\n",
        "            if class_file_counter > max_images_per_class:\n",
        "              break\n",
        "\n",
        "            file_path = os.path.join(directory, file)\n",
        "\n",
        "            if file.endswith(\".mp4\"):\n",
        "\n",
        "                vidcap = cv2.VideoCapture(file_path)\n",
        "\n",
        "                count = 0\n",
        "\n",
        "                real_count = 0\n",
        "\n",
        "                #we take a limited number of frames per second\n",
        "                while vidcap.isOpened():\n",
        "                    success, image = vidcap.read()\n",
        "                    if success and real_count < max_images_per_video and class_file_counter < max_images_per_class:\n",
        "                        \n",
        "                        detect_save_face(\n",
        "                            image, os.path.join(output_directories[directory_counter], \"%s.jpg\" % (\"frame_%d_%d\" % (file_counter, count))))\n",
        "                        count += 5 #we skip 5 frames per time\n",
        "                        vidcap.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
        "                       # print(output_directories[directory_counter] + \"%s.jpg\" % (\"frame_%d_%d\" % (file_counter, count)))\n",
        "\n",
        "                        class_file_counter+=1\n",
        "                        real_count+=1\n",
        "\n",
        "                        print(\"save face detected on the frame n. %d of the video n. %d\" % (count, file_counter))\n",
        "\n",
        "\n",
        "                    else:\n",
        "                        vidcap.release()\n",
        "                        break\n",
        "\n",
        "            elif file.endswith('.jpg') or file.endswith('.png') or file.endswith(\".jpeg\"):\n",
        "            \n",
        "                detect_save_face(cv2.imread(file_path), (os.path.join(output_directories[directory_counter], \"%s%s\" % (\n",
        "                    \"frame_%d\" % file_counter, get_file_type(file)))))\n",
        "                \n",
        "                print(\"save face detected on image n. %d\" % file_counter)\n",
        "                \n",
        "                class_file_counter += 1\n",
        "\n",
        "            file_counter += 1\n",
        "\n",
        "        directory_counter += 1\n",
        "\n",
        "\n",
        "take_images(mask=False) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tJoTeyWXjw"
      },
      "source": [
        "# TRAINING SECTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jORpzH_Quk2"
      },
      "source": [
        "## Get Training and Validation Data\n",
        "\n",
        "For each directory where we saved detected faces, we are going to create two arrays: **images** and **labels** (0 for bonafide, 1 for attackers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izwSR4HW0zbE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_images(directories, mask=False, balance_dataset=True, undersampling=False):\n",
        "\n",
        "    image_list = []\n",
        "    label_list = []\n",
        "\n",
        "    classes = 0\n",
        "\n",
        "    images_per_class=[]\n",
        "\n",
        "    tf_seed()\n",
        "\n",
        "    #for each input directory\n",
        "    for i in directories:\n",
        "\n",
        "        list_images = [f for f in os.listdir(i) if os.path.isfile(os.path.join(i, f))]\n",
        "\n",
        "        images_number = len(list_images)\n",
        "\n",
        "        print(\"n %d\" % images_number)\n",
        "\n",
        "        random.Random(20).shuffle(list_images)\n",
        "\n",
        "        #if we use undersampling\n",
        "        if undersampling:\n",
        "          images_number = images_number if ((len(images_per_class) == 0) or (images_number <= images_per_class[-1]))  else images_per_class[-1]\n",
        "        \n",
        "        \n",
        "        # to fill tensors we inizialise them\n",
        "        X = np.empty((images_number, final_x, final_y, n_channels))\n",
        "\n",
        "        L = np.empty((images_number))\n",
        "\n",
        "        ipp = 0\n",
        "\n",
        "        for im_name in list_images:\n",
        "            \n",
        "            im=os.path.join(i, im_name)\n",
        "\n",
        "            # to get \"balanced dataset\" using undersampling method\n",
        "            if (balance_dataset and undersampling) and len(images_per_class)  and ipp == images_per_class[-1]:\n",
        "                break\n",
        "\n",
        "            image = cv2.imread(im)\n",
        "            image=resize_normalize_image(image)\n",
        "\n",
        "          \n",
        "            if mask:\n",
        "              mask_name = os.path.join(\n",
        "                    i,\"mask\", get_file_name(im) + \"_mask\" + get_file_type(im))\n",
        "              image_mask = cv2.imread(mask_name, 0)\n",
        "              image_mask = resize_normalize_image(image_mask)\n",
        "                \n",
        "\n",
        "            X[ipp, ..., :3] = image # each original image is rgb\n",
        "            \n",
        "            if mask:\n",
        "              X[ipp,...,3] = image_mask # each mask can be n_channels - 3\n",
        "\n",
        "            L[ipp] = classes  # 0 for real, 1 for fake for binary classification\n",
        "\n",
        "            ipp += 1\n",
        "        \n",
        "        # check if the current class has lower images than the before\n",
        "        # in this case we undersample the majority class randomly removing elements\n",
        "        # we will have a balanced dataset: same images for all the classes\n",
        "\n",
        "        if balance_dataset and undersampling:\n",
        "            if len(images_per_class) and ipp < images_per_class[-1]:\n",
        "            \n",
        "              left_shift = 0\n",
        "\n",
        "              for i in range(0, len(images_per_class)):\n",
        "\n",
        "                  diff = images_per_class[i] - ipp\n",
        "\n",
        "                  for j in range(0, diff):\n",
        "                      random_index = np.random.randint(left_shift, images_per_class[i] - j)\n",
        "                      image_list.pop(random_index)\n",
        "                      label_list.pop(random_index)\n",
        "\n",
        "                  left_shift += images_per_class[i] - diff\n",
        "\n",
        "        if undersampling or balance_dataset==False:\n",
        "              for image in X:\n",
        "                image_list.append(image)\n",
        "\n",
        "              for label in L:\n",
        "                label_list.append(label)\n",
        "        else:\n",
        "              image_array=[]\n",
        "              label_array=[]\n",
        "            \n",
        "              for image in X:\n",
        "                image_array.append(image)\n",
        "\n",
        "              image_list.append(np.array(image_array, dtype=\"float\"))\n",
        "\n",
        "             \n",
        "              for label in L:\n",
        "                label_array.append(label)\n",
        "\n",
        "              label_list.append(label_array)\n",
        "\n",
        "              #[[2k], [1k]]\n",
        "        \n",
        "        if undersampling or balance_dataset==False:\n",
        "            images_per_class.append(ipp)\n",
        "        else:\n",
        "            images_per_class.append(images_number)\n",
        "\n",
        "        classes += 1\n",
        "\n",
        "    #offline augmentation\n",
        "    if balance_dataset and undersampling==False:\n",
        "\n",
        "      \n",
        "         max=np.max((images_per_class)) #[4k,3k,2k,1k]\n",
        "\n",
        "         for j in range(0, len(images_per_class)):  #[[4k], [4k], [4k], [4k]]\n",
        "            if images_per_class[j] < max:\n",
        "              diff=max - images_per_class[j] #how many images to generate\n",
        "\n",
        "              image_array=image_list[j]   #[3k]\n",
        "              label_array=label_list[j]   #[3k]\n",
        "\n",
        "              new_image_array=[]\n",
        "             \n",
        "\n",
        "              #use ImageDataGenerator \n",
        "              offline_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                       \twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\t                       horizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "              offline_generator=offline_generator.flow(image_array, batch_size=1, seed=20)\n",
        "\n",
        "              #generate the reamining images respect the majority class, to balance the dataset\n",
        "              generated_images=0\n",
        "              while generated_images < diff:\n",
        "                     \n",
        "                  for i in range(0,len(offline_generator)):\n",
        "                        if generated_images == diff:\n",
        "                          break\n",
        "                        batch= next(offline_generator)\n",
        "                        # print(batch.shape)\n",
        "                        #print(new_image_array.shape)\n",
        "                        new_image_array.append(batch[0])\n",
        "                        \n",
        "                        label_array.append(j)\n",
        "\n",
        "                      \n",
        "                        generated_images += 1\n",
        "\n",
        "     \n",
        "              image_list[j]=np.append(image_list[j], new_image_array, axis=0)\n",
        "\n",
        "\n",
        "         # image_list and label_list were of type [[...], [...]] for attack and bonafide classes\n",
        "         # we need a final array of type [image1,image2,...] for images and [0,1,...] for labels\n",
        "\n",
        "         new_image_list= np.empty((0, final_x, final_y, n_channels))\n",
        "         new_label_list=[]\n",
        "\n",
        "\n",
        "         for class_value_array in image_list:\n",
        "           new_image_list = np.vstack((new_image_list,class_value_array))\n",
        "\n",
        "         for class_value_array in label_list:\n",
        "           new_label_list += class_value_array\n",
        "\n",
        "         image_list = new_image_list\n",
        "         label_list = new_label_list\n",
        "\n",
        "         \n",
        "        # im_show(new_image_list[2]) # we can show up an image \n",
        "      \n",
        "        \n",
        "    #create the one-hot encoded vector: [0 1], [1 0]\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(label_list)\n",
        "    labels = tf.keras.utils.to_categorical(labels, 2)\n",
        "\n",
        "    if type(image_list) is np.ndarray:\n",
        "      return image_list, labels, le\n",
        "\n",
        "    return  np.array(image_list, dtype=\"float\"), labels, le\n",
        "\n",
        "\n",
        "\n",
        "read_from_np_array = True\n",
        "\n",
        "\n",
        "\n",
        "## WE CAN SAVE NUMPY RESULTS AND REUSE THEM LATER ##\n",
        "\n",
        "if read_from_np_array:\n",
        "      (train_X, test_X, train_Y, test_Y, final_test_X, final_test_Y) = pickle.loads(open(output_np_training_array, \"rb\").read())\n",
        "      le = pickle.loads(open(save_labels, \"rb\").read())\n",
        "\n",
        "## WE ARE GOING TO GET THE TRAIN AND VALIDATION DATASETS, AND WE SAVE THE NUMPY ARRAYS ##\n",
        "else:\n",
        "      \n",
        "      train_X, train_Y, label_encoder = get_images([output_training_real_folder, output_training_fake_folder], undersampling = True)\n",
        "\n",
        "      if delete_training_data_and_reset:\n",
        "        test_X, test_Y = ([], [])\n",
        "      else: \n",
        "        #will be ignored if we set split_randomly_training_and_validation variable to True\n",
        "        test_X, test_Y, _ = get_images([output_validation_real_folder, output_validation_fake_folder], undersampling=True)\n",
        "\n",
        "      final_test_X, final_test_Y, _ = get_images([output_test_real_folder,output_test_fake_folder], balance_dataset = False)\n",
        "      \n",
        "     # test_X=np.vstack((test_X,final_test_X))\n",
        "      #test_Y=np.vstack((test_Y,final_test_Y))\n",
        "\n",
        "\n",
        "      #save model labels \n",
        "      f = open(save_labels, \"wb\")\n",
        "      f.write(pickle.dumps(label_encoder))\n",
        "      f.close()\n",
        "\n",
        "      #save results\n",
        "      pickle.dump((train_X, test_X, train_Y, test_Y, final_test_X, final_test_Y), open(output_np_training_array, 'wb'))\n",
        "\n",
        "\n",
        "\n",
        "#print(\"max rgb value: %f\" % np.max(train_X))\n",
        "\n",
        "#will split entire dataset in 80% for training and 20% for validation\n",
        "if split_randomly_training_and_validation:\n",
        "\n",
        "  #the random seed for splitting \n",
        "\tsplit_seed=20\n",
        "\n",
        "\ttrainX, testX, trainY, testY = train_test_split(train_X, train_Y,\n",
        "\ttest_size=0.2, stratify=train_Y, seed=split_seed)\n",
        " \n",
        "else:\n",
        "\ttrainX, testX, trainY, testY = (train_X, test_X, train_Y, test_Y)\n",
        " \n",
        "print(\"Shape training: \", trainX.shape)\n",
        "print(\"Shape validation: \", testX.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_lCe8-YMFIb"
      },
      "source": [
        "## Augmentation Online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz29KTXlJJIi"
      },
      "outputs": [],
      "source": [
        "augmentation_online=True\n",
        "\n",
        "\n",
        "\n",
        "#the random seed for online augmentation\n",
        "generator_seed=10\n",
        "\n",
        " \n",
        "if augmentation_online:\n",
        "\t\t#online augmentation of training images\n",
        "\t\n",
        "\t\taug_training = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=30,\n",
        "                    zoom_range=0.35,\n",
        "                    width_shift_range=0.3, height_shift_range=0.3, \n",
        "shear_range=0.15,\n",
        "                    horizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "\t\t#aug_training = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\t#aug_training = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "\n",
        "\t\t# output an endless Iterator of batches\n",
        "\t\t# shuffle images, so the first generated batch is from batch_size random training images\n",
        "\n",
        "\t\ttrain_generator=aug_training.flow(trainX, trainY, batch_size=batch_size, shuffle=True, seed=generator_seed)\n",
        "\n",
        "\n",
        "\t\t# will be equal to np.ceil(len(train(X))/batch_size)\n",
        "\t\t# notice: if len(trainX) / batch_size is a float value, the last batch will contain less images than batch_size value\n",
        "\t\tprint(\"lenght training batches \", len(train_generator))\n",
        "\t\tprint(\"lenght training data \", len(trainX))\n",
        "\t\tprint(\"lenght validation data \", len(testX))\n",
        "\n",
        "\t\t# we could use also a validation_generator, without augment images\n",
        "\t\t# this \"trick\" could be done for set up a valdation batch size on model.fit().. then at the end of an epoch, each batch will be validated\n",
        "\n",
        "\t\t#aug_validation=ImageDataGenerator()\n",
        "\t\t#validation_generator=aug_validation.flow(testX, testY, batch_size=batch_size, shuffle=False, seed=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6BQ1i1eXj-D"
      },
      "source": [
        "## AUGMENTATION OFFLINE (OPTIONAL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVBP6_3baHSj"
      },
      "source": [
        "For each image in the training set, we're going to augment the image itself and we add it to the training. So if we have 2k images for the training, the final length of the dataset will be 4k."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvXvTIxcXnOb",
        "outputId": "8f9d0705-2771-4f6f-8fcc-d70b5a038e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape training:  (6714, 32, 32, 3)\n",
            "Shape validation:  (2418, 32, 32, 3)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "augmentation_online=False\n",
        "\n",
        "generator_seed=10\n",
        "     \n",
        "\n",
        "if not augmentation_online:\n",
        "\n",
        "\n",
        "  to_augment=len(trainY)  *2\n",
        "\n",
        "  #print(to_augment)\n",
        "  # to fill tensors we inizialise them\n",
        "  X = np.empty((to_augment, final_x, final_y, n_channels), dtype=\"float\")\n",
        "\n",
        "  L = np.empty((to_augment), dtype=\"int\")\n",
        "\n",
        "  #use ImageDataGenerator \n",
        "  offline_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
        "zoom_range=0.15,\n",
        "width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15,\n",
        "horizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "\n",
        "  offline_generator=offline_generator.flow(trainX, trainY, shuffle=True, batch_size=1, seed=generator_seed)\n",
        "\n",
        "  #for all initial images\n",
        "  for i in range(0,to_augment):\n",
        "            batchx, batchy = next(offline_generator)\n",
        "            X[i] = batchx[0]\n",
        "            L[i] = np.argmax(batchy[0])\n",
        "          \n",
        "  \n",
        "  labels = le.fit_transform(L)\n",
        "  labels = tf.keras.utils.to_categorical(labels, 2)\n",
        "\n",
        "  trainX = np.vstack((trainX, X))\n",
        "  trainY = np.vstack((trainY, labels))\n",
        "\n",
        "\n",
        "print(\"Shape training: \", trainX.shape)\n",
        "print(\"Shape validation: \", testX.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp8mvCvwLH5N"
      },
      "source": [
        "We can output the generator images organized in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "uE71Q14r1oRq",
        "outputId": "750edd94-bc5b-4ac2-d924-298fa6422646"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label-0: 0.0\n",
            "Label-1: 0.0\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# E' UN ESPERIMENTO SUL COMPORTAMENTO DI IMAGEDATAGENERATOR SU MODEL.FIT()\n",
        "\n",
        "# we generate batch_size images, beginning from the first batch\n",
        "batchx, batchy = next(train_generator)\n",
        "for i in range(batchx.shape[0]):\n",
        "    fig, (ax1) = plt.subplots(1) \n",
        "    ax1.set_title('image')  \n",
        "    ax1.imshow(batchx[i][:,:,0], cmap='gray')\n",
        "    print(\"Label-%d:\"%i, np.argmax(batchy[i]))\n",
        "\n",
        "\n",
        "train_generator.reset() #once an epoch ends, this function will be called to reset data\n",
        "\n",
        "\n",
        "\n",
        "# we generate exactly len(trainX) - batch_size images beginning from the first batch, since we have reset the train_generator on the line before\n",
        "for i in range(0,len(train_generator)):\n",
        " break # remove to see all training augmented images for an epoch\n",
        " batchx, batchy = next(train_generator)\n",
        " for i in range(batchx.shape[0]):\n",
        "    fig, (ax1) = plt.subplots(1) \n",
        "    ax1.set_title('image')  \n",
        "    ax1.imshow(batchx[i][:,:,0], cmap='gray')\n",
        "    print(\"Label-%d:\"%i, batchy[i][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yO34EpMQ8ov"
      },
      "source": [
        "## **Train the Model**\n",
        "\n",
        "We train the **model** and we save in a **CustomCallback** instance the average loss and accuracy for each training and validation batch. In this way, we can create a more defined training plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV09uMO-1jj7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch, psutil\n",
        "\n",
        "\n",
        "class MemoryUsage(tf.keras.callbacks.Callback):\n",
        "\n",
        "   def __init__(self):\n",
        "      # setting device on GPU if available, else CPU\n",
        "      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      print('Using device for training:', self.device)\n",
        "      self.max_RAM=[]\n",
        "      self.max_GPU=[]\n",
        "\n",
        "   def get_memory_usage(self):\n",
        "      gpu_dict = tf.config.experimental.get_memory_info('GPU:0')\n",
        "      tf.print('\\n GPU memory details [current: {} gb, peak: {} gb]'.format(\n",
        "          float(gpu_dict['current']) / (1024 ** 3), \n",
        "          float(gpu_dict['peak']) / (1024 ** 3)))\n",
        "   \n",
        "   def get_size(self, byte, suffix=\"GB\"):\n",
        "    factor = 1024\n",
        "    \n",
        "    for unit in [\"\", \"K\", \"M\", \"GB\", \"T\", \"P\"]:\n",
        "        if byte < factor:\n",
        "            return f\"{byte:.2f} GB\"\n",
        "        byte /= factor\n",
        "\n",
        "   def on_train_end(self, epoch, logs=None):\n",
        "      i=np.argmax(self.max_RAM)\n",
        "      j=np.argmin(self.max_RAM)\n",
        "      self.get_memory_usage()\n",
        "      print(\"MAX RAM USAGE: %s / %s (%s)\" % (self.get_size(self.max_RAM[i][0]), self.get_size(self.max_RAM[i][1]), str(self.max_RAM[i][2]) + \"%\" ))\n",
        "      \n",
        "\n",
        "\n",
        "   \n",
        "   def on_epoch_end(self,epoch,logs=None):\n",
        "      svmem = psutil.virtual_memory()\n",
        "      self.max_RAM.append((svmem.active, svmem.total, svmem.percent))\n",
        "      #self.get_memory_usage()\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.5)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.5)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate, decay=init_learning_rate/epochs)\n",
        "\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.99)\n",
        "tf_seed()\n",
        "model = LiveNet((final_y,final_x,n_channels))\n",
        "\n",
        "callback_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
        "memory_usage = MemoryUsage()\n",
        "\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])  # same of binary_crossentropy in this case\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#validation batch_size is equal to batch_size in this case (32)\n",
        "#check https://github.com/keras-team/keras/blob/v2.9.0/keras/engine/training.py#L1099-L1472\n",
        "if augmentation_online:\n",
        "    H=model.fit(\n",
        "      train_generator, \n",
        "      validation_data = (testX, testY), \n",
        "      callbacks = [memory_usage, callback_early_stopping],\n",
        "      epochs= epochs)\n",
        "else:\n",
        "   \n",
        "   H=model.fit(\n",
        "      trainX, trainY, \n",
        "      batch_size = batch_size,\n",
        "      shuffle=True,\n",
        "      validation_data = (testX, testY), \n",
        "      callbacks = [memory_usage, callback_early_stopping],\n",
        "      epochs= epochs)\n",
        "\n",
        "print(\"TRAINING TIME\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print((history.epoch_batch_train_acc[0])) #show first epoch batches averaged values\n",
        "\n",
        "print(\"[INFO] serializing network to '{}'...\".format(save_model_h5))\n",
        "model.save(save_model_h5, save_format=\"h5\")\n",
        "\n",
        "\n",
        "if not read_from_np_array:\n",
        "   #save model labels \n",
        "      f = open(save_labels, \"wb\")\n",
        "      f.write(pickle.dumps(label_encoder))\n",
        "      f.close()\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "fig=plt.figure(figsize=(12,7))\n",
        "\n",
        "\n",
        "\n",
        "new_epochs=len(H.history['loss'])\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "\n",
        "plt.savefig(save_training_metrics_plot)\n",
        "\n",
        "\n",
        "# notice accuracy at the end of each epoch is the mean all over the batches, the same if we have more than 1 validation batch\n",
        "\n",
        "#https://keras.io/guides/training_with_built_in_methods/\n",
        "#https://keras.io/getting_started/faq/#why-is-my-training-loss-much-higher-than-my-testing-loss\n",
        "#https://stackoverflow.com/questions/55097362/different-accuracy-by-fit-and-evaluate-in-keras-with-the-same-dataset\n",
        "\n",
        "# the below code can give never seen images at each validation_step; more in general we divide validation dataset in batches\n",
        "# see https://stackoverflow.com/questions/56991909/how-is-the-keras-accuracy-showed-in-progress-bar-calculated-from-which-inputs-i\n",
        "\n",
        "\n",
        "#model.fit(train_generator,\n",
        " # steps_per_epoch=len(train_generator),\n",
        "  #validation_data = validation_generator,\n",
        "\t#epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjlxdHfLMwi8"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWa2_Fpl4Sap"
      },
      "outputs": [],
      "source": [
        "#confusion matrix\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import itertools\n",
        "from itertools import cycle\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
        "\n",
        "model = tf.keras.models.load_model(save_model_h5)\n",
        "\n",
        "y_score = model.predict(testX)\n",
        "\n",
        "y_pred = np.argmax(y_score, axis = 1)\n",
        "y_test_ = np.argmax(testY, axis = 1)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \n",
        "    print(cm)\n",
        "  \n",
        "    def precision(index):\n",
        "      return round(cm[index][index] / cm[:, index].sum(),2)\n",
        "    \n",
        "    def recall(index):\n",
        "      return round(cm[index][index] /cm[index].sum(),2)\n",
        "    \n",
        "    def F1_score(index):\n",
        "      p=precision(index)\n",
        "      r=recall(index)\n",
        "      return round((2 * p * r)/(p + r),2)\n",
        "    \n",
        "    plt.figure(figsize=(6, 6), dpi=80)\n",
        "\n",
        "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, round(cm[i, j],2),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        \n",
        "   \n",
        "\n",
        "    #### CREATE THE PRECISION / RECALL / F1_SCORE TABLE ####\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 2 + 3 / 2.5))\n",
        "    \n",
        "    col_labels=[\"Bonafide\", \"Attacker\"]\n",
        "    row_labels=['Precision','Recall','F1 Score']\n",
        "    row_func=[precision,recall,F1_score]\n",
        "    table_vals=[]  \n",
        "\n",
        "    row_colors = np.full(len(row_labels), 'linen')\n",
        "    col_colors = np.full(len(col_labels), 'lavender')\n",
        "\n",
        "    \n",
        "    for i in range(0, len(row_labels)):\n",
        "\n",
        "      row=[]\n",
        "\n",
        "      for j in range(0, len(col_labels)):\n",
        "         row.append(row_func[i](j))\n",
        "\n",
        "      table_vals.append(row)\n",
        "    \n",
        "\n",
        "    # the rectangle is where I want to place the table\n",
        "    table = plt.table(cellText=table_vals,\n",
        "                  cellLoc='center',\n",
        "                  rowColours=row_colors,\n",
        "                  rowLabels=row_labels,\n",
        "                  rowLoc='center',\n",
        "                  colColours=col_colors,\n",
        "                  colLabels=col_labels,\n",
        "                  loc='center')\n",
        "    table.scale(1, 2)\n",
        "    ax1.axis('off')\n",
        "\n",
        "class_names = [\"Bonafide\", \"Attacker\"]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_, y_pred)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion Matrix')\n",
        "\n",
        "\n",
        "# we're using SoftMax\n",
        "sigmoid=False\n",
        "\n",
        "# if you use sigmoid as output, you can print one ROC Curve\n",
        "if sigmoid:\n",
        "    #ROC CURVE\n",
        "\n",
        "    fpr, tpr, _ =  roc_curve(test_Y, y_score)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(5, 5), dpi=80)\n",
        "    lw = 2\n",
        "\n",
        "    plt.plot(fpr, tpr, color='darkorange',lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver operating characteristic')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPHVyMuwNlDM"
      },
      "source": [
        "## Random Testing\n",
        "\n",
        "In this section we can test beginning from an image or using our webcam (to take one shot) for live detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BndKUCQRN67M"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "model = tf.keras.models.load_model(save_model_h5)\n",
        "le = pickle.loads(open(save_labels, \"rb\").read())\n",
        "\n",
        "def extract_and_predict_faces(image):\n",
        "\n",
        "    def translate(k):\n",
        "      if k == 0:\n",
        "        return \"Real\"\n",
        "      return \"Fake\"\n",
        "\n",
        "    extracted_test_faces, coordinates_faces = detect_save_face(\n",
        "    image, multiple_output=False)\n",
        "\n",
        "    for i in range(0, len(extracted_test_faces)):\n",
        "\n",
        "        face = extracted_test_faces[i]\n",
        "        (startX, startY, endX, endY) = coordinates_faces[i]\n",
        "\n",
        "        if face is not None:\n",
        "\n",
        "            #print(face)\n",
        "            face = resize_normalize_image(face)\n",
        "            face = face.astype(\"float\")\n",
        "            face = tf.keras.utils.img_to_array(face)\n",
        "            face = np.expand_dims(face, axis=0)\n",
        "            start_time = time.time()\n",
        "\n",
        "            preds = model.predict(face)[0]\n",
        "\n",
        "            print(\"--- inference in seconds : %f ---\" % (time.time() - start_time))\n",
        "\n",
        "\n",
        "            print(preds)\n",
        "            j = np.argmax(preds)\n",
        "            label = le.classes_[j]\n",
        "            \n",
        "            \n",
        "\n",
        "            # draw the label and bounding box on the frame\n",
        "            label = \"{}: {:.4f}\".format(translate(label), preds[j])\n",
        "            print(label)\n",
        "\n",
        "           \n",
        "            cv2.putText(image, label, (startX - 150, startY + 55),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 5, (0, 0, 255), 5)\n",
        "            cv2.rectangle(image, (startX, startY), (endX, endY),\n",
        "                      (0, 0, 255), 20)\n",
        "\n",
        "def im_show(image, output=None):\n",
        "   fig = plt.figure(figsize=(10, 10)) # set the height and width in inches\n",
        "   plt.imshow(image, cmap=\"Greys\")\n",
        "   plt.axis(\"off\")\n",
        "   plt.show()\n",
        "\n",
        "   if output is not None:\n",
        "     cv2.imwrite(output, image)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UL8NodskCIm-"
      },
      "outputs": [],
      "source": [
        "\n",
        "image_for_test = os.path.join(root_folder,'test3.jpg') \n",
        "\n",
        "face_confidence=0.16\n",
        "\n",
        "\n",
        "#we use an image we choose\n",
        "tested_image = cv2.imread(image_for_test)\n",
        "\n",
        "extract_and_predict_faces(tested_image)\n",
        "\n",
        "im_show(tested_image, \"test5.jpg\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPMCD45LOYsc"
      },
      "source": [
        "In the code below we access to the webcam to take a frame and predict it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyFjD-bHSvgt"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9NRv_nTSvgv"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "try:\n",
        "  filename = take_photo()\n",
        "  \n",
        "  \n",
        "  # Show the image which was just taken.\n",
        "  image=cv2.imread(filename)\n",
        " \n",
        "  extract_and_predict_faces(image)\n",
        "\n",
        "  im_show(image)\n",
        "\n",
        "  cv2.imwrite(\"photo.png\", image)\n",
        "\n",
        "  print('Saved to {}'.format(\"photo.png\"))\n",
        "\n",
        "except Exception as err:\n",
        "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
        "  # grant the page permission to access it.\n",
        "  print(str(err))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}