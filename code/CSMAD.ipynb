{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irqzKVH55WNX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aFsc5HPUO-u"
      },
      "source": [
        "We import all the main important **dependencies** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoiW3DW47srm"
      },
      "outputs": [],
      "source": [
        "import random, os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import shutil\n",
        "\n",
        "\n",
        "def tf_seed(seed=0):\n",
        "  np.random.seed(seed)  # numpy seed\n",
        "  tf.random.set_seed(seed)  # tensorflow seed\n",
        "  random.seed(seed)  # random seed\n",
        "  os.environ['TF_DETERMINISTIC_OPS'] = \"1\"\n",
        "  os.environ['TF_CUDNN_DETERMINISM'] = \"1\"\n",
        "  os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "  \n",
        "\n",
        "\n",
        "tf_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEFINE **NETWORK**"
      ],
      "metadata": {
        "id": "QSvO7Qoq9ktj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UxygEbcUYe6"
      },
      "source": [
        "The main idea is to classify a **bonafide or attack** human face with a fast **CNN**, setting on the top of the network an **SSD** model, pretrained on a **ResNet**, that can extract faces from images.\n",
        "\n",
        "The network will have two main convolutional steps, in which we are going to extract deep features from the second phase. It uses in sequence **Conv2d + BatchNormalization** to learn faster, normalizing batch feature maps to avoid **overfitting**. However, this is why is important to set a right batch size and training data to avoid the opposite problem, then underfitting. The BatchNormalization, since this is a shallow network, won't have the so called vanishing gradient problem during the first training epochs.\n",
        "\n",
        "There are also, at the end of each phase, a Dropout layer. This is because it's important to learn as much details as possible to distinguish a \"real or fake\" person in front of a camera. So switching off weights during training randomly, will have the consequence to distribute better the learning of important features among all the weights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8JXHbf1NAkl"
      },
      "source": [
        "## Originary Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PARxPl2btms"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(input)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, activation = 'relu', padding = 'same')(x)\n",
        "\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(64, activation = 'relu')(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)  #to avoid overfitting \n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekUqG6Q_NI4W"
      },
      "source": [
        "## AttackNet v2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT_0AQV2Hk0O"
      },
      "outputs": [],
      "source": [
        "def LiveNet(input_size):\n",
        "\n",
        "  chan_dim=-1 #feature map channel\n",
        "\n",
        "  #first convolutional phase\n",
        "  input = tf.keras.layers.Input(input_size)\n",
        "  y = tf.keras.layers.Conv2D(16, 3, padding = 'same')(input)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.Conv2D(16, 3, padding = 'same')(x)\n",
        "  z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.add([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #second convolutional phase\n",
        "  y = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.LeakyReLU(alpha=0.2)(y)\n",
        "  x = tf.keras.layers.BatchNormalization( axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  x = tf.keras.layers.Conv2D(32, 3, padding = 'same')(x)\n",
        "  z = tf.keras.layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = tf.keras.layers.add([y,z])\n",
        "  x = tf.keras.layers.BatchNormalization(axis=chan_dim)(x)\n",
        "  x = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "  x = tf.keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "  #dense phase\n",
        "  x = tf.keras.layers.Flatten()(x)\n",
        "  x = tf.keras.layers.Dense(128, activation = 'tanh')(x)\n",
        "  #x = BatchNormalization()(x)\n",
        "  x = tf.keras.layers.Dropout(0.5)(x)\n",
        "  #softmax \n",
        "  output = tf.keras.layers.Dense(2, activation = 'softmax')(x)\n",
        " \n",
        "  return tf.keras.models.Model(input, output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF972weGK-zC"
      },
      "source": [
        "# CSMAD DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqP77ScjVv-q"
      },
      "source": [
        "We define absolute paths that will be used.\n",
        "\n",
        "The dataset is the following: https://www.idiap.ch/en/dataset/csmad/index_html\n",
        "\n",
        "Sushil Bhattacharjee, Amir Mohammadi and Sebastien Marcel: \"Spoofing Deep Face Recognition With Custom Silicone Masks.\" in Proceedings of International Conference on Biometrics: Theory, Applications, and Systems (BTAS), 2018.\n",
        "10.1109/BTAS.2018.8698550\n",
        "http://publications.idiap.ch/index.php/publications/show/3887\n",
        "\n",
        "<br><br>\n",
        "Face-biometric data has been collected from 14 subjects to create this dataset. Subjects participating in this data-collection have played three roles: **targets**, **attackers**, and **bona-fide** clients. The subjects represented in the dataset are referred to here with letter-codes:** A .. N**. The subjects A..F have also been targets. That is, face-data for these six subjects has been used to construct their corresponding flexible masks (made of silicone). These masks have been made by Nimba Creations Ltd., a special effects company.\n",
        "\n",
        "## Dataset Structure\n",
        "\n",
        "We get a subset of the original dataset, so in our case is organized in two subdirectories: **attack**, **bonafide**. The two directories: **attack** and **bonafide** contain images for attacks and bona fide presentations, respectively. \n",
        "\n",
        "The folder **attack/WEAR** contains videos where the attack has been made by a person (attacker) wearing the mask of the target being attacked. The **attack/STAND** folder contains videos where the attack has been made using a the targetâ€™s mask mounted on an appropriate stand.\n",
        "\n",
        "## Training and Validation Data\n",
        "\n",
        "We can define which subject to use for attack-bonafide classes for training and validation. \n",
        "Setting the variable **split_randomly_training_and_validation**\n",
        "to **true** will **ignore** validation chosen data, and **will split the entire dataset in 70% for training, 30% for validation (it is not a correct way, but you can see how easy you reach a good validation accuracy)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lC86JJ3fNwfk"
      },
      "source": [
        "## Define Root Folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZXrDHVfVAOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3590b183-3132-47c4-8a52-ee63713f50c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attack folders for training:  [['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/A/Mask_atk_A1_i0_001/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/A/Mask_atk_A1_i0_001/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/A/Mask_atk_A1_i0_001/depth'], ['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/A/Mask_atk_A2_i1_007/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/A/Mask_atk_A2_i1_007/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/A/Mask_atk_A2_i1_007/depth'], ['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/C/Mask_atk_C1_i0_018/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/C/Mask_atk_C1_i0_018/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/C/Mask_atk_C1_i0_018/depth'], ['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/C/Mask_atk_C2_i0_024/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/C/Mask_atk_C2_i0_024/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/STAND/C/Mask_atk_C2_i0_024/depth'], ['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/E_atk_A1_i0_001/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/E_atk_A1_i0_001/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/E_atk_A1_i0_001/depth'], ['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/F_atk_A1_i0_011/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/F_atk_A1_i0_011/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/F_atk_A1_i0_011/depth'], ['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/E_atk_A1_i1_002/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/E_atk_A1_i1_002/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/A/E_atk_A1_i1_002/depth'], ['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/C/E_atk_C1_i0_017/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/C/E_atk_C1_i0_017/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/C/E_atk_C1_i0_017/depth'], ['/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/C/F_atk_C1_i0_027/color', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/C/F_atk_C1_i0_027/aligned_color_to_depth', '/content/drive/MyDrive/liveness_project/dataset/training/CSMAD extracted/attack/WEAR/C/F_atk_C1_i0_027/depth']]\n"
          ]
        }
      ],
      "source": [
        "root_folder=\"/content/drive/MyDrive/liveness_project/\"\n",
        "\n",
        "\n",
        "protoPath=os.path.join(root_folder,'face_detector/deploy.prototxt.txt')\n",
        "modelPath = os.path.join(root_folder,\n",
        "\t\"face_detector/res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "\n",
        "face_confidence=0.68\n",
        "\n",
        "CSMAD_extracted=os.path.join(root_folder,'dataset/training/CSMAD extracted/')\n",
        "\n",
        "all_subjects=[\"A\",\"B\",\"C\",\"D\"] #we have a,b,c,d on both bona fide and attacker folders; for bonafide we have more subjects to increase number of testing data\n",
        "\n",
        "attack_training_stand_labels=[\"A\",\"c\"]\n",
        "attack_training_wear_labels=[\"A\",\"c\"]\n",
        "\n",
        "attack_validation_stand_labels=[\"b\"]\n",
        "attack_validation_wear_labels=[\"b\",\"d\"]\n",
        "\n",
        "bonifade_training_labels=[\"A\", \"C\",\"H\",\"E\",\"F\",\"G\"]\n",
        "bonifade_validation_labels=[\"B\",\"D\",\"I\"]\n",
        "\n",
        "split_randomly_training_and_validation=False\n",
        "\n",
        "#will ignore validation data since all dataset will be splitted in 70% for training and 30% for validation\n",
        "if split_randomly_training_and_validation:\n",
        "\tattack_training_stand_labels=all_subjects\n",
        "\tattack_training_wear_labels=all_subjects\n",
        "\tbonifade_training_labels=all_subjects\n",
        "\n",
        "\tattack_validation_stand_labels=[]\n",
        "\tattack_validation_wear_labels=[]\n",
        "\tbonifade_validation_labels=[]\n",
        "\n",
        "\n",
        "\n",
        "#we define where to save output detected faces and the corresponding depth\n",
        "output_attack_training_folder=os.path.join(root_folder,'dataset/training/CSMAD/images/attack_training/')\n",
        "output_attack_validation_folder=os.path.join(root_folder,'dataset/training/CSMAD/images/attack_validation/')\n",
        "\n",
        "output_bonifade_training_folder=os.path.join(root_folder,'dataset/training/CSMAD/images/bonifade_training/')\n",
        "output_bonifade_validation_folder=os.path.join(root_folder,'dataset/training/CSMAD/images/bonifade_validation/')\n",
        "\n",
        "\n",
        "save_augmented_images =  os.path.join(root_folder,'dataset/training/CSMAD/augmented_images/') #output of online augmented images \n",
        "save_model_h5 = os.path.join(root_folder,'model_CSMAD.h5') \n",
        "save_labels = os.path.join(root_folder,'model_CSMAD_labels')\n",
        "save_training_metrics_plot = os.path.join(root_folder,'CSMAD_plot.png')\n",
        "\n",
        "output_np_training_array = os.path.join(root_folder,'CSMAD_training_array')\n",
        "\n",
        "\n",
        "\n",
        "def get_folders(root, labels):\n",
        "\tfolders=[]\n",
        "\n",
        "\tfor label in labels:\n",
        "\t\tnew_root = os.path.join(root,label.upper())\n",
        "\t\tnew_directories = [f.path for f in os.scandir(new_root) if f.is_dir()]\n",
        "\t\tfor directory in new_directories:\n",
        "\t\t\tsublist=[]\n",
        "\t\t\tsublist.append(os.path.join(new_root,directory, \"color\"))\n",
        "\t\t\tsublist.append(os.path.join(new_root,directory, \"aligned_color_to_depth\"))\n",
        "\t\t\tsublist.append(os.path.join(new_root,directory, \"depth\"))\n",
        "\t\t\tfolders.append(sublist)\n",
        "\t \n",
        "\treturn folders\n",
        "\n",
        "\n",
        "attack_folders_training=get_folders(os.path.join(CSMAD_extracted,'attack','STAND'), attack_training_stand_labels) + get_folders(os.path.join(CSMAD_extracted,'attack','WEAR'), attack_training_wear_labels)\n",
        "attack_folders_validation=get_folders(os.path.join(CSMAD_extracted,'attack','STAND'), attack_validation_stand_labels) + get_folders(os.path.join(CSMAD_extracted,'attack','WEAR'), attack_validation_wear_labels)\n",
        "\n",
        "bonifade_folders_training=get_folders(os.path.join(CSMAD_extracted,'bonifade'), bonifade_training_labels)\n",
        "bonifade_folders_validation=get_folders(os.path.join(CSMAD_extracted,'bonifade'), bonifade_validation_labels)\n",
        "\n",
        "\n",
        "training_directories=[attack_folders_training, attack_folders_validation, bonifade_folders_training, bonifade_folders_validation]\n",
        "output_directories=[output_attack_training_folder, output_attack_validation_folder, output_bonifade_training_folder, output_bonifade_validation_folder]\n",
        "\n",
        "\n",
        "#create directories where to save images if they don't exist\n",
        "for output_dir in output_directories:\n",
        "\tif not os.path.exists(output_dir): \n",
        "\t  os.makedirs(output_dir)\n",
        "\t  print(\"created \",  output_dir)\n",
        "\t \n",
        "\tmask_dir = os.path.join(output_dir ,\"mask\")\n",
        "\tif not os.path.exists(mask_dir): \n",
        "\t  os.makedirs(mask_dir)\n",
        "\t \n",
        "print(\"Attack folders for training: \" , attack_folders_training)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HE6q3yKLb16"
      },
      "source": [
        "# NETWORK PARAMETERS FOR TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La8H1nBNILps"
      },
      "outputs": [],
      "source": [
        "final_x = 32\n",
        "final_y = 32\n",
        "\n",
        "n_channels = 3 #rgb\n",
        "\n",
        "dim = (final_x, final_y)\n",
        "\n",
        "epochs=20\n",
        "batch_size=32\n",
        "init_learning_rate= 1e-5\n",
        "\n",
        "#return normalized image\n",
        "def resize_normalize_image(image, max_value = 255):\n",
        "\n",
        "    image=cv2.resize(image, dim)\n",
        "    return  (image)/ max_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzeH6X9BLKLe"
      },
      "source": [
        "# FACE PROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjPSQ7cRG9-F"
      },
      "source": [
        "## Network to Detect Faces\n",
        "\n",
        "We define the SSD network that detects face inside an image.\n",
        "\n",
        "In this case, since depth mask is not aligned with the original rgb image, we need to take the aligned color to depth image for each color image, in this way we can then concatenate the depth to the rgb channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNYKW76cU8KY"
      },
      "outputs": [],
      "source": [
        "\n",
        "net = cv2.dnn.readNetFromCaffe(protoPath, modelPath)\n",
        "\n",
        "def im_show(image, size=(15,15), output=None):\n",
        "   fig = plt.figure(figsize=size) # set the height and width in inches\n",
        "   plt.imshow(image, cmap=\"Greys\", interpolation=\"nearest\")\n",
        "   plt.axis(\"off\")\n",
        "   plt.show()\n",
        "\n",
        "   if output is not None:\n",
        "     cv2.imwrite(output, image)\n",
        "\n",
        "# detect the most probable face, or all of detected face\n",
        "# return coordinates of bounding box for each face and the face itself as a cv2 object\n",
        "def detect_save_face(color, align_to_depth, depth, output_path=None, mask_outputh=None, multiple_output=False):\n",
        "\n",
        "    (h, w) = color.shape[:2] \n",
        "    (h2,w2) = align_to_depth.shape[:2]\n",
        "\n",
        "\n",
        "    # we preprocess image normalizing each r-g-b channel subtracting the values in the last tuple\n",
        "    blob1 = cv2.dnn.blobFromImage(cv2.resize(color, (300, 300)), 1.0,\n",
        "                                 (300, 300), (104.0, 177.0, 123.0))\n",
        "    \n",
        "  \n",
        "    blob2 = cv2.dnn.blobFromImage(cv2.resize(align_to_depth, (300, 300)), 1.0,\n",
        "                                 (300, 300), (104.0, 177.0, 123.0))\n",
        "    \n",
        "    # pass the blob through the network and obtain the detections and\n",
        "    # predictions\n",
        "    net.setInput(blob1)\n",
        "    detections1 = net.forward()\n",
        "\n",
        "    # we need to detect the face also in the align_to_depth image\n",
        "    net.setInput(blob2)\n",
        "    detections2 = net.forward()\n",
        "\n",
        "    faces = []\n",
        "    coordinates=[]\n",
        "\n",
        "\n",
        "    # ensure at least one face was found\n",
        "    # ensure that same number of faces were found in color and aligned_to_depth images\n",
        "    if len(detections1) == len(detections2) and len(detections1) > 0:\n",
        "\n",
        "    \n",
        "        max_i=np.argmax(detections1[0, 0, :, 2]) #this is the most probable detected face\n",
        "\n",
        "        #if we want to output the most probable detected face or all the faces\n",
        "        min_range = 0 if multiple_output==True else max_i \n",
        "        max_range = detections1.shape[2] if multiple_output==True else max_i + 1\n",
        "\n",
        "        for i in range(min_range, max_range):\n",
        "\n",
        "             # we get the model confidence\n",
        "              confidence = detections1[0, 0, i, 2]\n",
        "\n",
        "              if confidence >= face_confidence:\n",
        "               \n",
        "                # compute the (x, y)-coordinates of the bounding box for\n",
        "                # the face and extract the face ROI\n",
        "                box = detections1[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "\n",
        "                # compute the (x, y)-coordinates of the bounding box for\n",
        "                # the face and extract the depth ROI\n",
        "                box1 = detections2[0, 0, i, 3:7] * np.array([w2, h2, w2, h2])\n",
        "                (startX2, startY2, endX2, endY2) = box1.astype(\"int\")\n",
        "\n",
        "                # we save coordinates \n",
        "                coordinates.append(box.astype(\"int\"))\n",
        "                face = color[startY:endY, startX:endX]\n",
        "                face_mask = depth[startY2:endY2, startX2:endX2]\n",
        "\n",
        "                \n",
        "\n",
        "                #we save the face\n",
        "                faces.append(face)\n",
        "                \n",
        "              \n",
        "                try:\n",
        "                    if mask_outputh is not None:\n",
        "                          cv2.imwrite(mask_outputh, face_mask)\n",
        "          \n",
        "                    if output_path is not None:\n",
        "                        cv2.imwrite(output_path, face)\n",
        "                except:\n",
        "                  pass\n",
        "                    \n",
        "             \n",
        "\n",
        "    return faces, coordinates\n",
        "\n",
        "#frame_mask_output = [mask_genuine_folder, mask_fake_folder]\n",
        "\n",
        "#return file name given a path\n",
        "def get_file_name(path):\n",
        "    base = os.path.basename(path)\n",
        "    return os.path.splitext(base)[0]\n",
        "\n",
        "#return file extension given a path\n",
        "def get_file_type(path): \n",
        "    base = os.path.basename(path)\n",
        "    return os.path.splitext(base)[1] # ex. .png, .jpg\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-z2RBn3fImps"
      },
      "source": [
        "## Extract Faces from Images\n",
        "\n",
        "We are going to take videos or images from our dataset. For each frame will be detected **one** face, and this one will be saved to the output folders defined before. The code reserves the possibility for the user to define also a mask for each image. In this case, the detected face will be also \"cropped\" from the mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QPqFZrOtse2"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_images_per_folder=200000 #we want to take a maximum number of images for training since limited hardware resources\n",
        "\n",
        "\n",
        "#for each training input directory, search for color image\n",
        "#for each color image (RGB), we take the corresponding align_to_depth and depth images \n",
        "#for each image and depth, we detect one face\n",
        "def take_images():\n",
        "    directory_counter = 0\n",
        "    file_counter = 0\n",
        "\n",
        "    for main_list in training_directories:\n",
        "\n",
        "          folder_file_counter=0\n",
        "\n",
        "          d_index=0\n",
        "\n",
        "          print(\"we're on array %d/%d\" % (directory_counter + 1, len(training_directories)))\n",
        "\n",
        "\n",
        "          for directory in main_list:\n",
        "\n",
        "            d_index+=1\n",
        "\n",
        "          # files = [file for file in os.listdir(directory[0])]\n",
        "\n",
        "            list_images = list(paths.list_images(directory[0]))\n",
        "\n",
        "            print(\"found %d images for folder n. %d/%d\" % (len(list_images), d_index, len(main_list)))\n",
        "\n",
        "\n",
        "            #we shuffle training data since we just take a limited number of images\n",
        "            random.Random(20).shuffle(list_images)\n",
        "\n",
        "\n",
        "            for file_path in list_images:\n",
        "\n",
        "                if folder_file_counter > max_images_per_folder:\n",
        "                  break\n",
        "\n",
        "              #  file_path = os.path.join(directory, file)\n",
        "                file=get_file_name(file_path) + get_file_type(file_path)\n",
        "\n",
        "                aligned_to_depth_image=cv2.imread(os.path.join(directory[1], file))\n",
        "                depth_image=cv2.imread(os.path.join(directory[2], file))\n",
        "                color=cv2.imread(file_path)\n",
        "\n",
        "              \n",
        "              \n",
        "                detect_save_face(color,  aligned_to_depth_image, depth_image, (output_directories[directory_counter] + \"%s%s\" % (\n",
        "                        \"frame_%d\" % file_counter, get_file_type(file))), (output_directories[directory_counter] + \"/mask/%s%s\" % (\n",
        "                        \"frame_%d_mask\" % file_counter, get_file_type(file))))\n",
        "                    \n",
        "                folder_file_counter+=1\n",
        "\n",
        "                print(\"save face detected on image n. %d, for folder n. %d\" % (file_counter, directory_counter + 1))\n",
        "\n",
        "\n",
        "                file_counter += 1\n",
        "\n",
        "          directory_counter += 1\n",
        "\n",
        "\n",
        "take_images() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EfcLzIHfNhAE"
      },
      "source": [
        "\n",
        "# TRAINING SECTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59tJoTeyWXjw"
      },
      "source": [
        "## Get Training and Validation Data\n",
        "\n",
        "For each directory where we saved detected faces, we are going to create two arrays: images, that will be concatenated with the depth mask, **by default**, and labels (0 for bonafide, 1 for attackers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izwSR4HW0zbE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_images(directories, mask=True, balance_dataset=True, undersampling=False):\n",
        "\n",
        "    image_list = []\n",
        "    label_list = []\n",
        "\n",
        "    classes = 0\n",
        "\n",
        "    images_per_class=[]\n",
        "\n",
        "    tf_seed()\n",
        "\n",
        "    #for each input directory\n",
        "    for i in directories:\n",
        "\n",
        "        list_images = [f for f in os.listdir(i) if os.path.isfile(os.path.join(i, f))]\n",
        "\n",
        "        images_number = len(list_images)\n",
        "\n",
        "        random.Random(20).shuffle(list_images)\n",
        "\n",
        "        print(\"n %d\" % images_number)\n",
        "\n",
        "        #if we use undersampling\n",
        "        if undersampling:\n",
        "          images_number = images_number if ((len(images_per_class) == 0) or (images_number <= images_per_class[-1]))  else images_per_class[-1]\n",
        "        \n",
        "        \n",
        "        # to fill tensors we inizialise them\n",
        "        X = np.empty((images_number, final_x, final_y, n_channels))\n",
        "\n",
        "        L = np.empty((images_number))\n",
        "\n",
        "        ipp = 0\n",
        "\n",
        "        for im_name in list_images:\n",
        "            \n",
        "            im=os.path.join(i, im_name)\n",
        "\n",
        "\n",
        "            # to get \"balanced dataset\" using undersampling method\n",
        "            if (balance_dataset and undersampling) and len(images_per_class)  and ipp == images_per_class[-1]:\n",
        "                break\n",
        "\n",
        "            image = cv2.imread(im)\n",
        "            image=resize_normalize_image(image)\n",
        "\n",
        "          \n",
        "            if mask:\n",
        "              mask_name = os.path.join(\n",
        "                    i,\"mask\", get_file_name(im) + \"_mask\" + get_file_type(im))\n",
        "              image_mask = cv2.imread(mask_name, 0)\n",
        "              image_mask = resize_normalize_image(image_mask)\n",
        "                \n",
        "\n",
        "            X[ipp, ..., :3] = image # each original image is rgb\n",
        "            \n",
        "            if mask:\n",
        "              X[ipp,...,3] = image_mask # each mask can be n_channels - 3\n",
        "\n",
        "            L[ipp] = classes  # 0 for real, 1 for fake for binary classification\n",
        "\n",
        "            ipp += 1\n",
        "        \n",
        "        # check if the current class has lower images than the before\n",
        "        # in this case we undersample the majority class randomly removing elements\n",
        "        # we will have a balanced dataset: same images for all the classes\n",
        "\n",
        "        if balance_dataset and undersampling:\n",
        "            if len(images_per_class) and ipp < images_per_class[-1]:\n",
        "            \n",
        "              left_shift = 0\n",
        "\n",
        "              for i in range(0, len(images_per_class)):\n",
        "\n",
        "                  diff = images_per_class[i] - ipp\n",
        "\n",
        "                  for j in range(0, diff):\n",
        "                      random_index = np.random.randint(left_shift, images_per_class[i] - j)\n",
        "                      image_list.pop(random_index)\n",
        "                      label_list.pop(random_index)\n",
        "                    #  print(random_index)\n",
        "\n",
        "                  left_shift += images_per_class[i] - diff\n",
        "\n",
        "        if undersampling or balance_dataset==False:\n",
        "              for image in X:\n",
        "                image_list.append(image)\n",
        "\n",
        "              for label in L:\n",
        "                label_list.append(label)\n",
        "        else:\n",
        "              image_array=[]\n",
        "              label_array=[]\n",
        "            \n",
        "              for image in X:\n",
        "                image_array.append(image)\n",
        "\n",
        "              image_list.append(np.array(image_array, dtype=\"float\"))\n",
        "\n",
        "             \n",
        "              for label in L:\n",
        "                label_array.append(label)\n",
        "\n",
        "              label_list.append(label_array)\n",
        "\n",
        "              #[[2k], [1k]]\n",
        "        \n",
        "        if undersampling or balance_dataset==False:\n",
        "            images_per_class.append(ipp)\n",
        "        else:\n",
        "            images_per_class.append(images_number)\n",
        "\n",
        "        classes += 1\n",
        "\n",
        "    #offline augmentation\n",
        "    if balance_dataset and undersampling==False:\n",
        "\n",
        "      \n",
        "         max=np.max((images_per_class)) #[4k,3k,2k,1k]\n",
        "\n",
        "         for j in range(0, len(images_per_class)):  #[[4k], [4k], [4k], [4k]]\n",
        "            if images_per_class[j] < max:\n",
        "              diff=max - images_per_class[j] #how many images to generate\n",
        "\n",
        "              image_array=image_list[j]   #[3k]\n",
        "              label_array=label_list[j]   #[3k]\n",
        "\n",
        "              new_image_array=[]\n",
        "             \n",
        "\n",
        "              #use ImageDataGenerator \n",
        "              offline_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, zoom_range=0.15,\n",
        "                       \twidth_shift_range=0.2, height_shift_range=0.2, shear_range=0.15,\n",
        "\t                       horizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "              offline_generator=offline_generator.flow(image_array, batch_size=1, seed = 2)\n",
        "\n",
        "              #generate the reamining images respect the majority class, to balance the dataset\n",
        "              generated_images=0\n",
        "              while generated_images < diff:\n",
        "                     \n",
        "                  for i in range(0,len(offline_generator)):\n",
        "                        if generated_images == diff:\n",
        "                          break\n",
        "                        batch= next(offline_generator)\n",
        "                        # print(batch.shape)\n",
        "                        #print(new_image_array.shape)\n",
        "                        new_image_array.append(batch[0])\n",
        "                        \n",
        "                        label_array.append(j)\n",
        "\n",
        "                      \n",
        "                        generated_images += 1\n",
        "\n",
        "            \n",
        "     \n",
        "              image_list[j]=np.append(image_list[j], new_image_array, axis=0)\n",
        "\n",
        "\n",
        "         # image_list and label_list were of type [[...], [...]] for attack and bonafide classes\n",
        "         # we need a final array of type [image1,image2,...] for images and [0,1,...] for labels\n",
        "\n",
        "         new_image_list= np.empty((0, final_x, final_y, n_channels))\n",
        "         new_label_list=[]\n",
        "\n",
        "\n",
        "         for class_value_array in image_list:\n",
        "           new_image_list = np.vstack((new_image_list,class_value_array))\n",
        "\n",
        "         for class_value_array in label_list:\n",
        "           new_label_list += class_value_array\n",
        "\n",
        "         image_list = new_image_list\n",
        "         label_list = new_label_list\n",
        "\n",
        "         print(image_list.shape)\n",
        "         print(len(label_list))\n",
        "         \n",
        "        # im_show(new_image_list[2]) # we can show up an image \n",
        "      \n",
        "        \n",
        "    #create the one-hot encoded vector: [0 1], [1 0]\n",
        "    le = LabelEncoder()\n",
        "    labels = le.fit_transform(label_list)\n",
        "    labels = tf.keras.utils.to_categorical(labels, 2)\n",
        "\n",
        "    if type(image_list) is np.ndarray:\n",
        "      return image_list, labels, le\n",
        "\n",
        "    return  np.array(image_list, dtype=\"float\"), labels, le\n",
        "\n",
        "\n",
        "\n",
        "read_from_np_array = True\n",
        "\n",
        "\n",
        "## WE CAN SAVE NUMPY RESULTS AND REUSE THEM LATER ##\n",
        "\n",
        "if read_from_np_array:\n",
        "      (train_X, test_X, train_Y, test_Y) = pickle.loads(open(output_np_training_array, \"rb\").read())\n",
        "      le = pickle.loads(open(save_labels, \"rb\").read())\n",
        "\n",
        "## WE ARE GOING TO GET THE TRAIN AND VALIDATION DATASETS, AND WE SAVE THE NUMPY ARRAYS ##\n",
        "else:\n",
        "      \n",
        "      train_X, train_Y, le = get_images([output_bonifade_training_folder, output_attack_training_folder], mask = True, undersampling=True)\n",
        "\n",
        "     \n",
        "      #will be ignored if we set split_randomly_training_and_validation variable to True\n",
        "      test_X, test_Y, _ = get_images([output_bonifade_validation_folder, output_attack_validation_folder], mask = True, undersampling=True)\n",
        "\n",
        "\n",
        "      #save model labels \n",
        "      f = open(save_labels, \"wb\")\n",
        "      f.write(pickle.dumps(le))\n",
        "      f.close()\n",
        "\n",
        "      #save results\n",
        "      pickle.dump((train_X, test_X, train_Y, test_Y), open(output_np_training_array, 'wb'))\n",
        "\n",
        "\n",
        "#will split entire dataset in 80% for training and 30% for validation\n",
        "if split_randomly_training_and_validation:\n",
        "\n",
        "  #the random seed for splitting \n",
        "\tsplit_seed=20\n",
        "\n",
        "\ttrainX, testX, trainY, testY = train_test_split(train_X, train_Y,\n",
        "\ttest_size=0.3, stratify=train_Y, seed=split_seed)\n",
        " \n",
        "else:\n",
        "\ttrainX, testX, trainY, testY = (train_X, test_X, train_Y, test_Y)\n",
        " \n",
        "if n_channels == 3:\n",
        "  trainX = trainX[...,:3]\n",
        "  testX = testX[...,:3]\n",
        " \n",
        "print(\"Shape training: \", trainX.shape)\n",
        "print(\"Shape validation: \", testX.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_lCe8-YMFIb"
      },
      "source": [
        "## Augmentation Online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz29KTXlJJIi"
      },
      "outputs": [],
      "source": [
        "augmentation_online=True\n",
        "\n",
        "\n",
        "\n",
        "#the random seed for online augmentation\n",
        "generator_seed=10\n",
        "\n",
        " \n",
        "if augmentation_online:\n",
        "\t\t#online augmentation of training images\n",
        "\t\taug_training = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20, \n",
        "zoom_range=0.25,\n",
        " width_shift_range=0.1, height_shift_range=0.1, shear_range=0.15,\n",
        " horizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\t#aug_training = tf.keras.preprocessing.image.ImageDataGenerator()\n",
        "\n",
        "\n",
        "\t\t# output an endless Iterator of batches\n",
        "\t\t# shuffle images, so the first generated batch is from batch_size random training images\n",
        "\n",
        "\t\ttrain_generator=aug_training.flow(trainX, trainY, batch_size=batch_size, shuffle=True, seed=generator_seed)\n",
        "\n",
        "\n",
        "\t\t# will be equal to np.ceil(len(train(X))/batch_size)\n",
        "\t\t# notice: if len(trainX) / batch_size is a float value, the last batch will contain less images than batch_size value\n",
        "\t\tprint(\"lenght training batches \", len(train_generator))\n",
        "\t\tprint(\"lenght training data \", len(trainX))\n",
        "\t\tprint(\"lenght validation data \", len(testX))\n",
        "\n",
        "\t\t# we could use also a validation_generator, without augment images\n",
        "\t\t# this \"trick\" could be done for set up a valdation batch size on model.fit().. then at the end of an epoch, each batch will be validated\n",
        "\n",
        "\t\t#aug_validation=ImageDataGenerator()\n",
        "\t\t#validation_generator=aug_validation.flow(testX, testY, batch_size=batch_size, shuffle=False, seed=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp8mvCvwLH5N"
      },
      "source": [
        "We can output the generator images organized in batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uE71Q14r1oRq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# we generate batch_size images, beginning from the first batch\n",
        "batchx, batchy = next(train_generator)\n",
        "for i in range(batchx.shape[0]):\n",
        "    fig, (ax1) = plt.subplots(1) \n",
        "    ax1.set_title('image')  \n",
        "    ax1.imshow(batchx[i][:,:,0], cmap='gray')\n",
        "    print(\"Label-%d:\"%i, np.argmax(batchy[i]))\n",
        "\n",
        "\n",
        "train_generator.reset() #once an epoch ends, this function will be called to reset data\n",
        "\n",
        "# we generate exactly len(trainX) - batch_size images beginning from the first batch, since we have reset the train_generator on the line before\n",
        "for i in range(0,len(train_generator) - 1):\n",
        " break # remove to see all training augmented images for an epoch\n",
        " batchx, batchy = next(train_generator)\n",
        " for i in range(batchx.shape[0]):\n",
        "    fig, (ax1) = plt.subplots(1) \n",
        "    ax1.set_title('image')  \n",
        "    ax1.imshow(batchx[i][:,:,0], cmap='gray')\n",
        "    print(\"Label-%d:\"%i, batchy[i][0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OFFLINE AUGMENTATION (optional)"
      ],
      "metadata": {
        "id": "rtE7tE8ezNw9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each image in the training set, we're going to augment the image itself and we add it to the training. So if we have 2k images for the training, the final length of the dataset will be 4k."
      ],
      "metadata": {
        "id": "GVBP6_3baHSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "augmentation_online=False\n",
        "\n",
        "generator_seed=10\n",
        "     \n",
        "\n",
        "if not augmentation_online:\n",
        "\n",
        "\n",
        "  to_augment=len(trainY)\n",
        "  #print(to_augment)\n",
        "  # to fill tensors we inizialise them\n",
        "  X = np.empty((to_augment, final_x, final_y, n_channels), dtype=\"float\")\n",
        "\n",
        "  L = np.empty((to_augment), dtype=\"int\")\n",
        "\n",
        "  #use ImageDataGenerator \n",
        "  offline_generator = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=30,\n",
        "zoom_range=0.25,\n",
        "width_shift_range=0.3, height_shift_range=0.3, shear_range=0.15,\n",
        "horizontal_flip=True, fill_mode=\"constant\")\n",
        "\n",
        "\n",
        "\n",
        "  offline_generator=offline_generator.flow(trainX, trainY, shuffle=True, batch_size=1, seed=generator_seed)\n",
        "\n",
        "  #for all initial images\n",
        "  for i in range(0,to_augment):\n",
        "            batchx, batchy = next(offline_generator)\n",
        "            X[i] = batchx[0]\n",
        "            L[i] = np.argmax(batchy[0])\n",
        "          \n",
        "  \n",
        "  labels = le.fit_transform(L)\n",
        "  labels = tf.keras.utils.to_categorical(labels, 2)\n",
        "\n",
        "  trainX = np.vstack((trainX, X))\n",
        "  trainY = np.vstack((trainY, labels))\n",
        "\n",
        "\n",
        "print(\"Shape training: \", trainX.shape)\n",
        "print(\"Shape validation: \", testX.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "HvXvTIxcXnOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu2_1UwcLVuF"
      },
      "source": [
        "## **Train the Model**\n",
        "\n",
        "We train the **model** and we save in a **CustomCallback** instance the average loss and accuracy for each training and validation batch. In this way, we can create a more defined training plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV09uMO-1jj7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch, psutil\n",
        "\n",
        "\n",
        "class MemoryUsage(tf.keras.callbacks.Callback):\n",
        "\n",
        "   def __init__(self):\n",
        "      # setting device on GPU if available, else CPU\n",
        "      self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      print('Using device for training:', self.device)\n",
        "      self.max_RAM=[]\n",
        "      self.max_GPU=[]\n",
        "\n",
        "   def get_memory_usage(self):\n",
        "      gpu_dict = tf.config.experimental.get_memory_info('GPU:0')\n",
        "      tf.print('\\n GPU memory details [current: {} gb, peak: {} gb]'.format(\n",
        "          float(gpu_dict['current']) / (1024 ** 3), \n",
        "          float(gpu_dict['peak']) / (1024 ** 3)))\n",
        "   \n",
        "   def get_size(self, byte, suffix=\"GB\"):\n",
        "    factor = 1024\n",
        "    \n",
        "    for unit in [\"\", \"K\", \"M\", \"GB\", \"T\", \"P\"]:\n",
        "        if byte < factor:\n",
        "            return f\"{byte:.2f} GB\"\n",
        "        byte /= factor\n",
        "\n",
        "   def on_train_end(self, epoch, logs=None):\n",
        "      i=np.argmax(self.max_RAM)\n",
        "      self.get_memory_usage()\n",
        "      print(\"MAX RAM USAGE: %s / %s (%s)\" % (self.get_size(self.max_RAM[i][0]), self.get_size(self.max_RAM[i][1]), str(self.max_RAM[i][2]) + \"%\" ))\n",
        "      \n",
        "\n",
        "\n",
        "   \n",
        "   def on_epoch_end(self,epoch,logs=None):\n",
        "      svmem = psutil.virtual_memory()\n",
        "      self.max_RAM.append((svmem.active, svmem.total, svmem.percent))\n",
        "      #self.get_memory_usage()\n",
        "      \n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "\n",
        "# initialize the optimizer and model\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.5)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate)\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.5)\n",
        "#opt = tf.keras.optimizers.Adam(learning_rate=init_learning_rate, decay=init_learning_rate/epochs)\n",
        "\n",
        "\n",
        "#opt = tf.keras.optimizers.SGD(learning_rate=init_learning_rate, momentum = 0.99)\n",
        "tf_seed()\n",
        "model = LiveNet((final_y,final_x,n_channels))\n",
        "\n",
        "callback_early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10)\n",
        "memory_usage = MemoryUsage()\n",
        "\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])  # same of binary_crossentropy in this case\n",
        "\n",
        "\n",
        "\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#validation batch_size is equal to batch_size in this case (32)\n",
        "#check https://github.com/keras-team/keras/blob/v2.9.0/keras/engine/training.py#L1099-L1472\n",
        "if augmentation_online:\n",
        "    H=model.fit(\n",
        "      train_generator, \n",
        "      validation_data = (testX, testY), \n",
        "      callbacks = [memory_usage, callback_early_stopping],\n",
        "      epochs= epochs)\n",
        "else:\n",
        "   \n",
        "   H=model.fit(\n",
        "      trainX, trainY, \n",
        "      batch_size = batch_size,\n",
        "      shuffle=True,\n",
        "      validation_data = (testX, testY), \n",
        "      callbacks = [memory_usage, callback_early_stopping],\n",
        "      epochs= epochs)\n",
        "\n",
        "print(\"TRAINING TIME\")\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print((history.epoch_batch_train_acc[0])) #show first epoch batches averaged values\n",
        "\n",
        "print(\"[INFO] serializing network to '{}'...\".format(save_model_h5))\n",
        "model.save(save_model_h5, save_format=\"h5\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "fig=plt.figure(figsize=(12,7))\n",
        "\n",
        "\n",
        "\n",
        "new_epochs=len(H.history['loss'])\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(np.arange(0, new_epochs), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "\n",
        "\n",
        "plt.savefig(save_training_metrics_plot)\n",
        "\n",
        "\n",
        "# notice accuracy at the end of each epoch is the mean all over the batches, the same if we have more than 1 validation batch\n",
        "\n",
        "#https://keras.io/guides/training_with_built_in_methods/\n",
        "#https://keras.io/getting_started/faq/#why-is-my-training-loss-much-higher-than-my-testing-loss\n",
        "#https://stackoverflow.com/questions/55097362/different-accuracy-by-fit-and-evaluate-in-keras-with-the-same-dataset\n",
        "\n",
        "# the below code can give never seen images at each validation_step; more in general we divide validation dataset in batches\n",
        "# see https://stackoverflow.com/questions/56991909/how-is-the-keras-accuracy-showed-in-progress-bar-calculated-from-which-inputs-i\n",
        "\n",
        "\n",
        "#model.fit(train_generator,\n",
        " # steps_per_epoch=len(train_generator),\n",
        "  #validation_data = validation_generator,\n",
        "\t#epochs=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjlxdHfLMwi8"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GdTpA07N4Xrl"
      },
      "outputs": [],
      "source": [
        "#confusion matrix\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
        "import itertools\n",
        "from itertools import cycle\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
        "\n",
        "model = tf.keras.models.load_model(save_model_h5)\n",
        "\n",
        "\n",
        "y_score = model.predict(testX)\n",
        "\n",
        "y_pred = np.argmax(y_score, axis = 1)\n",
        "y_test_ = np.argmax(testY, axis = 1)\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "  \n",
        "    def precision(index):\n",
        "      return round(cm[index][index] / cm[:, index].sum(),2)\n",
        "    \n",
        "    def recall(index):\n",
        "      return round(cm[index][index] /cm[index].sum(),2)\n",
        "    \n",
        "    def F1_score(index):\n",
        "      p=precision(index)\n",
        "      r=recall(index)\n",
        "      return round((2 * p * r)/(p + r),2)\n",
        "    \n",
        "    plt.figure(figsize=(6, 6), dpi=80)\n",
        "\n",
        "    im = plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, round(cm[i, j],2),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        \n",
        "   \n",
        "\n",
        "    #### CREATE THE PRECISION / RECALL / F1_SCORE TABLE ####\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "    fig, ax1 = plt.subplots(figsize=(10, 2 + 3 / 2.5))\n",
        "    \n",
        "    col_labels=[\"Bonafide\", \"Attacker\"]\n",
        "    row_labels=['Precision','Recall','F1 Score']\n",
        "    row_func=[precision,recall,F1_score]\n",
        "    table_vals=[]  \n",
        "\n",
        "    row_colors = np.full(len(row_labels), 'linen')\n",
        "    col_colors = np.full(len(col_labels), 'lavender')\n",
        "\n",
        "    \n",
        "    for i in range(0, len(row_labels)):\n",
        "\n",
        "      row=[]\n",
        "\n",
        "      for j in range(0, len(col_labels)):\n",
        "         row.append(row_func[i](j))\n",
        "\n",
        "      table_vals.append(row)\n",
        "    \n",
        "\n",
        "    # the rectangle is where I want to place the table\n",
        "    table = plt.table(cellText=table_vals,\n",
        "                  cellLoc='center',\n",
        "                  rowColours=row_colors,\n",
        "                  rowLabels=row_labels,\n",
        "                  rowLoc='center',\n",
        "                  colColours=col_colors,\n",
        "                  colLabels=col_labels,\n",
        "                  loc='center')\n",
        "    table.scale(1, 2)\n",
        "    ax1.axis('off')\n",
        "\n",
        "    \n",
        "\n",
        "class_names = [\"Bonafide\", \"Attacker\"]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test_, y_pred)\n",
        "\n",
        "# Plot non-normalized confusion matrix\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion Matrix')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}